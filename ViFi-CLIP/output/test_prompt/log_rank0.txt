[2025-03-09 20:45:42 ViT-B/16] (main.py 348): INFO working dir: output/test_prompt
[2025-03-09 20:45:42 ViT-B/16] (main.py 352): INFO AUG:
  COLOR_JITTER: 0.8
  CUTMIX: 1.0
  GRAY_SCALE: 0.2
  LABEL_SMOOTH: 0.1
  MIXUP: 0.8
  MIXUP_SWITCH_PROB: 0.5
BASE: ['']
DATA:
  DATASET: humanedit
  INPUT_SIZE: 224
  LABEL_LIST: /home/jovyan/BA/Github/thesis-edit-evaluation/data/humanedit/labels.csv
  NUM_CLASSES: 4534
  NUM_FRAMES: 2
  ROOT: /home/jovyan/BA/Github/HumanEdit/videos
  TRAIN_FILE: /home/jovyan/BA/Github/thesis-edit-evaluation/data/humanedit/5f_cv/train_fold_1.txt
  VAL_FILE: /home/jovyan/BA/Github/thesis-edit-evaluation/data/humanedit/5f_cv/test_fold_1.txt
LOCAL_RANK: 0
MODEL:
  ARCH: ViT-B/16
  DROP_PATH_RATE: 0.0
  FIX_TEXT: True
  PRETRAINED: None
  RESUME: ckts/k400_seed1_vifi_clip_base2novel.pth
OUTPUT: output/test_prompt
PRINT_FREQ: 50
SAVE_FREQ: 10
SEED: 1024
TEST:
  MULTI_VIEW_INFERENCE: False
  NUM_CLIP: 1
  NUM_CROP: 1
  ONLY_TEST: False
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: False
  BATCH_SIZE: 16
  EPOCHS: 11
  LR: 0.04
  LR_SCHEDULER: cosine
  OPTIMIZER: adamw
  OPT_LEVEL: O1
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 5
  WEIGHT_DECAY: 0.001
TRAINER:
  ViFi_CLIP:
    CTX_INIT: a photo of a
    N_CTX_TEXT: 16
    N_CTX_VISION: 16
    PROMPT_DEPTH_TEXT: 9
    PROMPT_DEPTH_VISION: 9
    PROMPT_MODEL: True
    USE: both
    ZS_EVAL: False
VAL_FREQ: 1
[2025-03-09 20:45:42 ViT-B/16] (vificlip.py 232): INFO Loading CLIP (backbone: ViT-B/16)
[2025-03-09 20:45:44 ViT-B/16] (vificlip.py 235): INFO Building ViFi-CLIP CLIP
[2025-03-09 20:45:44 ViT-B/16] (vificlip.py 97): INFO V-L design
[2025-03-09 20:45:44 ViT-B/16] (vificlip.py 98): INFO Initial text context: "X X X X X X X X X X X X X X X X"
[2025-03-09 20:45:44 ViT-B/16] (vificlip.py 99): INFO Number of context words (tokens) for Language prompting: 16
[2025-03-09 20:45:44 ViT-B/16] (vificlip.py 100): INFO Number of context words (tokens) for Vision prompting: 16
[2025-03-09 20:45:44 ViT-B/16] (vificlip.py 239): INFO Turning off gradients in both the image and the text encoder
[2025-03-09 20:45:44 ViT-B/16] (vificlip.py 276): INFO Total learnable items: 18
[2025-03-09 20:45:45 ViT-B/16] (tools.py 66): INFO ==============> Resuming form ckts/k400_seed1_vifi_clip_base2novel.pth....................
[2025-03-09 20:46:01 ViT-B/16] (tools.py 81): INFO resume model: _IncompatibleKeys(missing_keys=['module.clip_model.positional_embedding', 'module.clip_model.text_projection', 'module.clip_model.logit_scale', 'module.clip_model.visual.VPT', 'module.clip_model.visual.class_embedding', 'module.clip_model.visual.positional_embedding', 'module.clip_model.visual.proj', 'module.clip_model.visual.conv1.weight', 'module.clip_model.visual.ln_pre.weight', 'module.clip_model.visual.ln_pre.bias', 'module.clip_model.visual.transformer.resblocks.0.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.0.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.0.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.0.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.0.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.0.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.1.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.1.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.1.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.1.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.1.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.1.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.1.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.1.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.2.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.2.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.2.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.2.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.2.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.2.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.2.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.2.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.2.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.3.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.3.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.3.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.3.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.3.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.3.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.3.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.3.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.3.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.4.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.4.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.4.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.4.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.4.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.4.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.4.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.4.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.4.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.5.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.5.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.5.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.5.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.5.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.5.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.5.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.5.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.5.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.6.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.6.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.6.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.6.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.6.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.6.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.6.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.6.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.6.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.7.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.7.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.7.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.7.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.7.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.7.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.7.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.7.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.7.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.8.VPT_shallow', 'module.clip_model.visual.transformer.resblocks.8.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.8.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.8.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.8.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.8.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.8.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.8.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.8.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.9.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.9.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.9.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.9.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.9.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.9.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.9.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.9.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.10.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.10.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.10.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.10.ln_2.bias', 'module.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight', 'module.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias', 'module.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight', 'module.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias', 'module.clip_model.visual.transformer.resblocks.11.ln_1.weight', 'module.clip_model.visual.transformer.resblocks.11.ln_1.bias', 'module.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'module.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'module.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'module.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'module.clip_model.visual.transformer.resblocks.11.ln_2.weight', 'module.clip_model.visual.transformer.resblocks.11.ln_2.bias', 'module.clip_model.visual.ln_post.weight', 'module.clip_model.visual.ln_post.bias', 'module.clip_model.transformer.resblocks.0.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.0.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.0.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.0.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.0.ln_1.weight', 'module.clip_model.transformer.resblocks.0.ln_1.bias', 'module.clip_model.transformer.resblocks.0.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.0.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.0.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.0.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.0.ln_2.weight', 'module.clip_model.transformer.resblocks.0.ln_2.bias', 'module.clip_model.transformer.resblocks.1.VPT_shallow', 'module.clip_model.transformer.resblocks.1.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.1.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.1.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.1.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.1.ln_1.weight', 'module.clip_model.transformer.resblocks.1.ln_1.bias', 'module.clip_model.transformer.resblocks.1.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.1.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.1.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.1.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.1.ln_2.weight', 'module.clip_model.transformer.resblocks.1.ln_2.bias', 'module.clip_model.transformer.resblocks.2.VPT_shallow', 'module.clip_model.transformer.resblocks.2.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.2.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.2.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.2.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.2.ln_1.weight', 'module.clip_model.transformer.resblocks.2.ln_1.bias', 'module.clip_model.transformer.resblocks.2.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.2.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.2.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.2.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.2.ln_2.weight', 'module.clip_model.transformer.resblocks.2.ln_2.bias', 'module.clip_model.transformer.resblocks.3.VPT_shallow', 'module.clip_model.transformer.resblocks.3.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.3.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.3.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.3.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.3.ln_1.weight', 'module.clip_model.transformer.resblocks.3.ln_1.bias', 'module.clip_model.transformer.resblocks.3.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.3.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.3.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.3.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.3.ln_2.weight', 'module.clip_model.transformer.resblocks.3.ln_2.bias', 'module.clip_model.transformer.resblocks.4.VPT_shallow', 'module.clip_model.transformer.resblocks.4.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.4.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.4.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.4.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.4.ln_1.weight', 'module.clip_model.transformer.resblocks.4.ln_1.bias', 'module.clip_model.transformer.resblocks.4.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.4.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.4.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.4.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.4.ln_2.weight', 'module.clip_model.transformer.resblocks.4.ln_2.bias', 'module.clip_model.transformer.resblocks.5.VPT_shallow', 'module.clip_model.transformer.resblocks.5.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.5.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.5.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.5.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.5.ln_1.weight', 'module.clip_model.transformer.resblocks.5.ln_1.bias', 'module.clip_model.transformer.resblocks.5.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.5.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.5.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.5.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.5.ln_2.weight', 'module.clip_model.transformer.resblocks.5.ln_2.bias', 'module.clip_model.transformer.resblocks.6.VPT_shallow', 'module.clip_model.transformer.resblocks.6.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.6.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.6.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.6.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.6.ln_1.weight', 'module.clip_model.transformer.resblocks.6.ln_1.bias', 'module.clip_model.transformer.resblocks.6.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.6.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.6.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.6.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.6.ln_2.weight', 'module.clip_model.transformer.resblocks.6.ln_2.bias', 'module.clip_model.transformer.resblocks.7.VPT_shallow', 'module.clip_model.transformer.resblocks.7.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.7.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.7.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.7.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.7.ln_1.weight', 'module.clip_model.transformer.resblocks.7.ln_1.bias', 'module.clip_model.transformer.resblocks.7.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.7.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.7.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.7.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.7.ln_2.weight', 'module.clip_model.transformer.resblocks.7.ln_2.bias', 'module.clip_model.transformer.resblocks.8.VPT_shallow', 'module.clip_model.transformer.resblocks.8.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.8.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.8.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.8.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.8.ln_1.weight', 'module.clip_model.transformer.resblocks.8.ln_1.bias', 'module.clip_model.transformer.resblocks.8.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.8.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.8.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.8.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.8.ln_2.weight', 'module.clip_model.transformer.resblocks.8.ln_2.bias', 'module.clip_model.transformer.resblocks.9.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.9.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.9.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.9.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.9.ln_1.weight', 'module.clip_model.transformer.resblocks.9.ln_1.bias', 'module.clip_model.transformer.resblocks.9.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.9.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.9.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.9.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.9.ln_2.weight', 'module.clip_model.transformer.resblocks.9.ln_2.bias', 'module.clip_model.transformer.resblocks.10.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.10.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.10.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.10.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.10.ln_1.weight', 'module.clip_model.transformer.resblocks.10.ln_1.bias', 'module.clip_model.transformer.resblocks.10.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.10.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.10.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.10.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.10.ln_2.weight', 'module.clip_model.transformer.resblocks.10.ln_2.bias', 'module.clip_model.transformer.resblocks.11.attn.in_proj_weight', 'module.clip_model.transformer.resblocks.11.attn.in_proj_bias', 'module.clip_model.transformer.resblocks.11.attn.out_proj.weight', 'module.clip_model.transformer.resblocks.11.attn.out_proj.bias', 'module.clip_model.transformer.resblocks.11.ln_1.weight', 'module.clip_model.transformer.resblocks.11.ln_1.bias', 'module.clip_model.transformer.resblocks.11.mlp.c_fc.weight', 'module.clip_model.transformer.resblocks.11.mlp.c_fc.bias', 'module.clip_model.transformer.resblocks.11.mlp.c_proj.weight', 'module.clip_model.transformer.resblocks.11.mlp.c_proj.bias', 'module.clip_model.transformer.resblocks.11.ln_2.weight', 'module.clip_model.transformer.resblocks.11.ln_2.bias', 'module.clip_model.token_embedding.weight', 'module.clip_model.ln_final.weight', 'module.clip_model.ln_final.bias', 'module.prompt_learner.ctx', 'module.prompt_learner.token_prefix', 'module.prompt_learner.token_suffix', 'module.image_encoder.VPT', 'module.image_encoder.transformer.resblocks.1.VPT_shallow', 'module.image_encoder.transformer.resblocks.2.VPT_shallow', 'module.image_encoder.transformer.resblocks.3.VPT_shallow', 'module.image_encoder.transformer.resblocks.4.VPT_shallow', 'module.image_encoder.transformer.resblocks.5.VPT_shallow', 'module.image_encoder.transformer.resblocks.6.VPT_shallow', 'module.image_encoder.transformer.resblocks.7.VPT_shallow', 'module.image_encoder.transformer.resblocks.8.VPT_shallow', 'module.text_encoder.transformer.resblocks.1.VPT_shallow', 'module.text_encoder.transformer.resblocks.2.VPT_shallow', 'module.text_encoder.transformer.resblocks.3.VPT_shallow', 'module.text_encoder.transformer.resblocks.4.VPT_shallow', 'module.text_encoder.transformer.resblocks.5.VPT_shallow', 'module.text_encoder.transformer.resblocks.6.VPT_shallow', 'module.text_encoder.transformer.resblocks.7.VPT_shallow', 'module.text_encoder.transformer.resblocks.8.VPT_shallow'], unexpected_keys=[])
[2025-03-09 20:46:04 ViT-B/16] (main.py 184): INFO Train: [0/11][0/296]	eta 0:16:06 lr 0.000000000	time 3.2636 (3.2636)	tot_loss 3.8201 (3.8201)	mem 6497MB
[2025-03-09 20:47:40 ViT-B/16] (main.py 184): INFO Train: [0/11][50/296]	eta 0:07:57 lr 0.001324324	time 2.0122 (1.9429)	tot_loss 3.7057 (3.7170)	mem 6497MB
[2025-03-09 20:49:21 ViT-B/16] (main.py 184): INFO Train: [0/11][100/296]	eta 0:06:28 lr 0.002675676	time 2.0252 (1.9819)	tot_loss 3.5230 (3.6609)	mem 6497MB
[2025-03-09 20:51:02 ViT-B/16] (main.py 184): INFO Train: [0/11][150/296]	eta 0:04:51 lr 0.004027027	time 2.0145 (1.9950)	tot_loss 3.7331 (3.6582)	mem 6497MB
[2025-03-09 20:52:43 ViT-B/16] (main.py 184): INFO Train: [0/11][200/296]	eta 0:03:12 lr 0.005378378	time 2.0137 (2.0002)	tot_loss 3.7983 (3.6470)	mem 6497MB
[2025-03-09 20:54:23 ViT-B/16] (main.py 184): INFO Train: [0/11][250/296]	eta 0:01:32 lr 0.006729730	time 2.0088 (2.0023)	tot_loss 3.8869 (3.6369)	mem 6497MB
[2025-03-09 20:55:54 ViT-B/16] (main.py 191): INFO EPOCH 0 training takes 0:09:53
[2025-03-09 20:55:54 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 21:05:24 ViT-B/16] (main.py 299): INFO Training
[2025-03-09 21:05:24 ViT-B/16] (main.py 300): INFO  * Loss: 8.4108
[2025-03-09 21:05:24 ViT-B/16] (main.py 301): INFO  * Mean Rank: 288.383, Median Rank: 59.000
[2025-03-09 21:05:24 ViT-B/16] (main.py 302): INFO  * Recall@1: 5.807, Recall@5: 17.504, Recall@10: 24.768, Recall@50: 47.065
[2025-03-09 21:05:24 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 21:23:50 ViT-B/16] (main.py 299): INFO Validation
[2025-03-09 21:23:50 ViT-B/16] (main.py 300): INFO  * Loss: 8.4102
[2025-03-09 21:23:50 ViT-B/16] (main.py 301): INFO  * Mean Rank: 259.191, Median Rank: 55.000
[2025-03-09 21:23:50 ViT-B/16] (main.py 302): INFO  * Recall@1: 6.745, Recall@5: 18.971, Recall@10: 27.150, Recall@50: 48.735
[2025-03-09 21:23:50 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 6.7%
[2025-03-09 21:23:50 ViT-B/16] (main.py 114): INFO Max accuracy: 6.75%
[2025-03-09 21:23:50 ViT-B/16] (tools.py 55): INFO output/test_prompt/ckpt_epoch_0.pth saving......
[2025-03-09 21:24:01 ViT-B/16] (tools.py 57): INFO output/test_prompt/ckpt_epoch_0.pth saved !!!
[2025-03-09 21:24:12 ViT-B/16] (tools.py 61): INFO output/test_prompt/best.pth saved !!!
[2025-03-09 21:24:15 ViT-B/16] (main.py 184): INFO Train: [1/11][0/296]	eta 0:14:38 lr 0.007972973	time 2.9677 (2.9677)	tot_loss 3.4952 (3.4952)	mem 6508MB
[2025-03-09 21:25:56 ViT-B/16] (main.py 184): INFO Train: [1/11][50/296]	eta 0:08:21 lr 0.009324324	time 2.0450 (2.0401)	tot_loss 3.6191 (3.5326)	mem 6508MB
[2025-03-09 21:27:38 ViT-B/16] (main.py 184): INFO Train: [1/11][100/296]	eta 0:06:39 lr 0.010675676	time 2.0272 (2.0373)	tot_loss 3.8398 (3.5685)	mem 6508MB
[2025-03-09 21:29:19 ViT-B/16] (main.py 184): INFO Train: [1/11][150/296]	eta 0:04:56 lr 0.012027027	time 2.0142 (2.0320)	tot_loss 3.7531 (3.5771)	mem 6508MB
[2025-03-09 21:31:00 ViT-B/16] (main.py 184): INFO Train: [1/11][200/296]	eta 0:03:14 lr 0.013378378	time 2.0130 (2.0279)	tot_loss 3.0377 (3.5906)	mem 6508MB
[2025-03-09 21:32:40 ViT-B/16] (main.py 184): INFO Train: [1/11][250/296]	eta 0:01:33 lr 0.014729730	time 2.0150 (2.0249)	tot_loss 3.5763 (3.5798)	mem 6508MB
[2025-03-09 21:34:11 ViT-B/16] (main.py 191): INFO EPOCH 1 training takes 0:09:58
[2025-03-09 21:34:11 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 21:43:42 ViT-B/16] (main.py 299): INFO Training
[2025-03-09 21:43:42 ViT-B/16] (main.py 300): INFO  * Loss: 8.4092
[2025-03-09 21:43:42 ViT-B/16] (main.py 301): INFO  * Mean Rank: 278.430, Median Rank: 54.000
[2025-03-09 21:43:42 ViT-B/16] (main.py 302): INFO  * Recall@1: 6.018, Recall@5: 18.285, Recall@10: 25.528, Recall@50: 49.050
[2025-03-09 21:43:42 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 22:02:07 ViT-B/16] (main.py 299): INFO Validation
[2025-03-09 22:02:07 ViT-B/16] (main.py 300): INFO  * Loss: 8.4082
[2025-03-09 22:02:07 ViT-B/16] (main.py 301): INFO  * Mean Rank: 257.668, Median Rank: 49.000
[2025-03-09 22:02:07 ViT-B/16] (main.py 302): INFO  * Recall@1: 6.745, Recall@5: 18.887, Recall@10: 28.246, Recall@50: 50.506
[2025-03-09 22:02:07 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 6.7%
[2025-03-09 22:02:07 ViT-B/16] (main.py 114): INFO Max accuracy: 6.75%
[2025-03-09 22:02:10 ViT-B/16] (main.py 184): INFO Train: [2/11][0/296]	eta 0:14:55 lr 0.015972973	time 3.0246 (3.0246)	tot_loss 3.5221 (3.5221)	mem 6508MB
[2025-03-09 22:03:51 ViT-B/16] (main.py 184): INFO Train: [2/11][50/296]	eta 0:08:18 lr 0.017324324	time 2.0057 (2.0271)	tot_loss 3.6681 (3.5386)	mem 6508MB
[2025-03-09 22:05:31 ViT-B/16] (main.py 184): INFO Train: [2/11][100/296]	eta 0:06:35 lr 0.018675676	time 2.0085 (2.0185)	tot_loss 3.5734 (3.5566)	mem 6508MB
[2025-03-09 22:07:12 ViT-B/16] (main.py 184): INFO Train: [2/11][150/296]	eta 0:04:54 lr 0.020027027	time 2.0135 (2.0155)	tot_loss 3.8050 (3.5619)	mem 6508MB
[2025-03-09 22:08:52 ViT-B/16] (main.py 184): INFO Train: [2/11][200/296]	eta 0:03:13 lr 0.021378378	time 2.0101 (2.0140)	tot_loss 3.6707 (3.5547)	mem 6508MB
[2025-03-09 22:10:33 ViT-B/16] (main.py 184): INFO Train: [2/11][250/296]	eta 0:01:32 lr 0.022729730	time 2.0112 (2.0132)	tot_loss 3.7385 (3.5501)	mem 6508MB
[2025-03-09 22:12:03 ViT-B/16] (main.py 191): INFO EPOCH 2 training takes 0:09:55
[2025-03-09 22:12:03 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 22:21:34 ViT-B/16] (main.py 299): INFO Training
[2025-03-09 22:21:34 ViT-B/16] (main.py 300): INFO  * Loss: 8.4071
[2025-03-09 22:21:34 ViT-B/16] (main.py 301): INFO  * Mean Rank: 260.610, Median Rank: 51.000
[2025-03-09 22:21:34 ViT-B/16] (main.py 302): INFO  * Recall@1: 6.060, Recall@5: 18.856, Recall@10: 27.027, Recall@50: 49.789
[2025-03-09 22:21:34 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 22:39:59 ViT-B/16] (main.py 299): INFO Validation
[2025-03-09 22:39:59 ViT-B/16] (main.py 300): INFO  * Loss: 8.4059
[2025-03-09 22:39:59 ViT-B/16] (main.py 301): INFO  * Mean Rank: 225.752, Median Rank: 48.000
[2025-03-09 22:39:59 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.673, Recall@5: 19.899, Recall@10: 27.572, Recall@50: 50.506
[2025-03-09 22:39:59 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 7.7%
[2025-03-09 22:39:59 ViT-B/16] (main.py 114): INFO Max accuracy: 7.67%
[2025-03-09 22:39:59 ViT-B/16] (tools.py 55): INFO output/test_prompt/ckpt_epoch_2.pth saving......
[2025-03-09 22:40:10 ViT-B/16] (tools.py 57): INFO output/test_prompt/ckpt_epoch_2.pth saved !!!
[2025-03-09 22:40:22 ViT-B/16] (tools.py 61): INFO output/test_prompt/best.pth saved !!!
[2025-03-09 22:40:25 ViT-B/16] (main.py 184): INFO Train: [3/11][0/296]	eta 0:14:44 lr 0.023972973	time 2.9882 (2.9882)	tot_loss 3.8462 (3.8462)	mem 6508MB
[2025-03-09 22:42:06 ViT-B/16] (main.py 184): INFO Train: [3/11][50/296]	eta 0:08:21 lr 0.025324324	time 2.0408 (2.0404)	tot_loss 3.1508 (3.5325)	mem 6508MB
[2025-03-09 22:43:47 ViT-B/16] (main.py 184): INFO Train: [3/11][100/296]	eta 0:06:39 lr 0.026675676	time 2.0272 (2.0374)	tot_loss 3.5569 (3.5421)	mem 6508MB
[2025-03-09 22:45:29 ViT-B/16] (main.py 184): INFO Train: [3/11][150/296]	eta 0:04:56 lr 0.028027027	time 2.0176 (2.0322)	tot_loss 3.3568 (3.5266)	mem 6508MB
[2025-03-09 22:47:09 ViT-B/16] (main.py 184): INFO Train: [3/11][200/296]	eta 0:03:14 lr 0.029378378	time 2.0123 (2.0281)	tot_loss 3.5339 (3.5267)	mem 6508MB
[2025-03-09 22:48:50 ViT-B/16] (main.py 184): INFO Train: [3/11][250/296]	eta 0:01:33 lr 0.030729730	time 2.0076 (2.0249)	tot_loss 3.5592 (3.5309)	mem 6508MB
[2025-03-09 22:50:21 ViT-B/16] (main.py 191): INFO EPOCH 3 training takes 0:09:58
[2025-03-09 22:50:21 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 22:59:51 ViT-B/16] (main.py 299): INFO Training
[2025-03-09 22:59:51 ViT-B/16] (main.py 300): INFO  * Loss: 8.4069
[2025-03-09 22:59:51 ViT-B/16] (main.py 301): INFO  * Mean Rank: 253.787, Median Rank: 48.000
[2025-03-09 22:59:51 ViT-B/16] (main.py 302): INFO  * Recall@1: 6.968, Recall@5: 19.721, Recall@10: 27.618, Recall@50: 50.612
[2025-03-09 22:59:51 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 23:18:17 ViT-B/16] (main.py 299): INFO Validation
[2025-03-09 23:18:17 ViT-B/16] (main.py 300): INFO  * Loss: 8.4064
[2025-03-09 23:18:17 ViT-B/16] (main.py 301): INFO  * Mean Rank: 220.127, Median Rank: 44.000
[2025-03-09 23:18:17 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.841, Recall@5: 21.501, Recall@10: 30.017, Recall@50: 52.782
[2025-03-09 23:18:17 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 7.8%
[2025-03-09 23:18:17 ViT-B/16] (main.py 114): INFO Max accuracy: 7.84%
[2025-03-09 23:18:17 ViT-B/16] (tools.py 55): INFO output/test_prompt/ckpt_epoch_3.pth saving......
[2025-03-09 23:18:28 ViT-B/16] (tools.py 57): INFO output/test_prompt/ckpt_epoch_3.pth saved !!!
[2025-03-09 23:18:39 ViT-B/16] (tools.py 61): INFO output/test_prompt/best.pth saved !!!
[2025-03-09 23:18:42 ViT-B/16] (main.py 184): INFO Train: [4/11][0/296]	eta 0:14:38 lr 0.031972973	time 2.9683 (2.9683)	tot_loss 3.3439 (3.3439)	mem 6508MB
[2025-03-09 23:20:23 ViT-B/16] (main.py 184): INFO Train: [4/11][50/296]	eta 0:08:22 lr 0.033324324	time 2.0436 (2.0411)	tot_loss 2.9507 (3.4716)	mem 6508MB
[2025-03-09 23:22:05 ViT-B/16] (main.py 184): INFO Train: [4/11][100/296]	eta 0:06:39 lr 0.034675676	time 2.0252 (2.0374)	tot_loss 3.3824 (3.5045)	mem 6508MB
[2025-03-09 23:23:46 ViT-B/16] (main.py 184): INFO Train: [4/11][150/296]	eta 0:04:56 lr 0.036027027	time 2.0144 (2.0321)	tot_loss 3.4768 (3.4932)	mem 6508MB
[2025-03-09 23:25:27 ViT-B/16] (main.py 184): INFO Train: [4/11][200/296]	eta 0:03:14 lr 0.037378378	time 2.0108 (2.0280)	tot_loss 3.7564 (3.5055)	mem 6508MB
[2025-03-09 23:27:07 ViT-B/16] (main.py 184): INFO Train: [4/11][250/296]	eta 0:01:33 lr 0.038729730	time 2.0087 (2.0247)	tot_loss 3.7651 (3.5016)	mem 6508MB
[2025-03-09 23:28:38 ViT-B/16] (main.py 191): INFO EPOCH 4 training takes 0:09:58
[2025-03-09 23:28:38 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 23:38:09 ViT-B/16] (main.py 299): INFO Training
[2025-03-09 23:38:09 ViT-B/16] (main.py 300): INFO  * Loss: 8.4056
[2025-03-09 23:38:09 ViT-B/16] (main.py 301): INFO  * Mean Rank: 243.670, Median Rank: 43.000
[2025-03-09 23:38:09 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.390, Recall@5: 20.186, Recall@10: 28.737, Recall@50: 52.703
[2025-03-09 23:38:09 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-09 23:56:33 ViT-B/16] (main.py 299): INFO Validation
[2025-03-09 23:56:33 ViT-B/16] (main.py 300): INFO  * Loss: 8.4054
[2025-03-09 23:56:33 ViT-B/16] (main.py 301): INFO  * Mean Rank: 206.854, Median Rank: 44.000
[2025-03-09 23:56:33 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.589, Recall@5: 21.585, Recall@10: 30.860, Recall@50: 52.530
[2025-03-09 23:56:33 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 7.6%
[2025-03-09 23:56:33 ViT-B/16] (main.py 114): INFO Max accuracy: 7.84%
[2025-03-09 23:56:37 ViT-B/16] (main.py 184): INFO Train: [5/11][0/296]	eta 0:15:11 lr 0.039972973	time 3.0810 (3.0810)	tot_loss 3.2272 (3.2272)	mem 6508MB
[2025-03-09 23:58:17 ViT-B/16] (main.py 184): INFO Train: [5/11][50/296]	eta 0:08:18 lr 0.022088449	time 2.0049 (2.0277)	tot_loss 3.5695 (3.4315)	mem 6508MB
[2025-03-09 23:59:57 ViT-B/16] (main.py 184): INFO Train: [5/11][100/296]	eta 0:06:35 lr 0.021135761	time 2.0118 (2.0186)	tot_loss 3.8481 (3.4369)	mem 6508MB
[2025-03-10 00:01:38 ViT-B/16] (main.py 184): INFO Train: [5/11][150/296]	eta 0:04:54 lr 0.020180896	time 2.0073 (2.0154)	tot_loss 3.7577 (3.4499)	mem 6508MB
[2025-03-10 00:03:18 ViT-B/16] (main.py 184): INFO Train: [5/11][200/296]	eta 0:03:13 lr 0.019226075	time 2.0082 (2.0136)	tot_loss 3.4494 (3.4510)	mem 6508MB
[2025-03-10 00:04:59 ViT-B/16] (main.py 184): INFO Train: [5/11][250/296]	eta 0:01:32 lr 0.018273520	time 2.0060 (2.0126)	tot_loss 3.2832 (3.4526)	mem 6508MB
[2025-03-10 00:06:29 ViT-B/16] (main.py 191): INFO EPOCH 5 training takes 0:09:55
[2025-03-10 00:06:29 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-10 00:15:59 ViT-B/16] (main.py 299): INFO Training
[2025-03-10 00:15:59 ViT-B/16] (main.py 300): INFO  * Loss: 8.4050
[2025-03-10 00:15:59 ViT-B/16] (main.py 301): INFO  * Mean Rank: 215.101, Median Rank: 40.000
[2025-03-10 00:15:59 ViT-B/16] (main.py 302): INFO  * Recall@1: 6.968, Recall@5: 20.608, Recall@10: 29.603, Recall@50: 53.526
[2025-03-10 00:15:59 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-10 00:34:24 ViT-B/16] (main.py 299): INFO Validation
[2025-03-10 00:34:24 ViT-B/16] (main.py 300): INFO  * Loss: 8.4040
[2025-03-10 00:34:24 ViT-B/16] (main.py 301): INFO  * Mean Rank: 190.745, Median Rank: 39.000
[2025-03-10 00:34:24 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.589, Recall@5: 22.260, Recall@10: 30.691, Recall@50: 54.890
[2025-03-10 00:34:24 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 7.6%
[2025-03-10 00:34:24 ViT-B/16] (main.py 114): INFO Max accuracy: 7.84%
[2025-03-10 00:34:27 ViT-B/16] (main.py 184): INFO Train: [6/11][0/296]	eta 0:14:51 lr 0.017401077	time 3.0106 (3.0106)	tot_loss 3.4904 (3.4904)	mem 6508MB
[2025-03-10 00:36:07 ViT-B/16] (main.py 184): INFO Train: [6/11][50/296]	eta 0:08:18 lr 0.016459079	time 2.0074 (2.0258)	tot_loss 3.4693 (3.4919)	mem 6508MB
[2025-03-10 00:37:48 ViT-B/16] (main.py 184): INFO Train: [6/11][100/296]	eta 0:06:35 lr 0.015525785	time 2.0049 (2.0168)	tot_loss 3.4366 (3.4984)	mem 6508MB
[2025-03-10 00:39:28 ViT-B/16] (main.py 184): INFO Train: [6/11][150/296]	eta 0:04:54 lr 0.014603368	time 2.0065 (2.0140)	tot_loss 3.0375 (3.4595)	mem 6508MB
[2025-03-10 00:41:09 ViT-B/16] (main.py 184): INFO Train: [6/11][200/296]	eta 0:03:13 lr 0.013693973	time 2.0088 (2.0125)	tot_loss 3.6659 (3.4538)	mem 6508MB
[2025-03-10 00:42:49 ViT-B/16] (main.py 184): INFO Train: [6/11][250/296]	eta 0:01:32 lr 0.012799719	time 2.0118 (2.0116)	tot_loss 3.3614 (3.4407)	mem 6508MB
[2025-03-10 00:44:20 ViT-B/16] (main.py 191): INFO EPOCH 6 training takes 0:09:55
[2025-03-10 00:44:20 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-10 00:53:50 ViT-B/16] (main.py 299): INFO Training
[2025-03-10 00:53:50 ViT-B/16] (main.py 300): INFO  * Loss: 8.4030
[2025-03-10 00:53:50 ViT-B/16] (main.py 301): INFO  * Mean Rank: 203.306, Median Rank: 37.000
[2025-03-10 00:53:50 ViT-B/16] (main.py 302): INFO  * Recall@1: 8.087, Recall@5: 22.551, Recall@10: 31.229, Recall@50: 54.730
[2025-03-10 00:53:50 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-10 01:12:14 ViT-B/16] (main.py 299): INFO Validation
[2025-03-10 01:12:14 ViT-B/16] (main.py 300): INFO  * Loss: 8.4029
[2025-03-10 01:12:14 ViT-B/16] (main.py 301): INFO  * Mean Rank: 187.184, Median Rank: 38.000
[2025-03-10 01:12:14 ViT-B/16] (main.py 302): INFO  * Recall@1: 8.685, Recall@5: 23.103, Recall@10: 30.691, Recall@50: 54.469
[2025-03-10 01:12:14 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 8.7%
[2025-03-10 01:12:14 ViT-B/16] (main.py 114): INFO Max accuracy: 8.68%
[2025-03-10 01:12:14 ViT-B/16] (tools.py 55): INFO output/test_prompt/ckpt_epoch_6.pth saving......
[2025-03-10 01:12:25 ViT-B/16] (tools.py 57): INFO output/test_prompt/ckpt_epoch_6.pth saved !!!
[2025-03-10 01:12:37 ViT-B/16] (tools.py 61): INFO output/test_prompt/best.pth saved !!!
[2025-03-10 01:12:40 ViT-B/16] (main.py 184): INFO Train: [7/11][0/296]	eta 0:14:52 lr 0.011992164	time 3.0147 (3.0147)	tot_loss 3.5970 (3.5970)	mem 6508MB
[2025-03-10 01:14:21 ViT-B/16] (main.py 184): INFO Train: [7/11][50/296]	eta 0:08:21 lr 0.011132776	time 2.0406 (2.0401)	tot_loss 3.5416 (3.4135)	mem 6508MB
[2025-03-10 01:16:02 ViT-B/16] (main.py 184): INFO Train: [7/11][100/296]	eta 0:06:39 lr 0.010294486	time 2.0241 (2.0360)	tot_loss 3.3909 (3.4234)	mem 6508MB
[2025-03-10 01:17:43 ViT-B/16] (main.py 184): INFO Train: [7/11][150/296]	eta 0:04:56 lr 0.009479245	time 2.0166 (2.0311)	tot_loss 3.5900 (3.4372)	mem 6508MB
[2025-03-10 01:19:24 ViT-B/16] (main.py 184): INFO Train: [7/11][200/296]	eta 0:03:14 lr 0.008688952	time 2.0121 (2.0267)	tot_loss 3.6682 (3.4384)	mem 6508MB
[2025-03-10 01:21:04 ViT-B/16] (main.py 184): INFO Train: [7/11][250/296]	eta 0:01:33 lr 0.007925444	time 2.0121 (2.0234)	tot_loss 3.4547 (3.4381)	mem 6508MB
[2025-03-10 01:22:35 ViT-B/16] (main.py 191): INFO EPOCH 7 training takes 0:09:58
[2025-03-10 01:22:35 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-10 01:32:05 ViT-B/16] (main.py 299): INFO Training
[2025-03-10 01:32:05 ViT-B/16] (main.py 300): INFO  * Loss: 8.4023
[2025-03-10 01:32:05 ViT-B/16] (main.py 301): INFO  * Mean Rank: 198.616, Median Rank: 34.000
[2025-03-10 01:32:05 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.897, Recall@5: 22.593, Recall@10: 31.820, Recall@50: 56.208
[2025-03-10 01:32:05 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-10 01:50:29 ViT-B/16] (main.py 299): INFO Validation
[2025-03-10 01:50:29 ViT-B/16] (main.py 300): INFO  * Loss: 8.4023
[2025-03-10 01:50:29 ViT-B/16] (main.py 301): INFO  * Mean Rank: 180.749, Median Rank: 37.000
[2025-03-10 01:50:29 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.841, Recall@5: 22.513, Recall@10: 32.040, Recall@50: 56.071
[2025-03-10 01:50:29 ViT-B/16] (main.py 111): INFO Accuracy of the network on the 1187 test videos: 7.8%
[2025-03-10 01:50:29 ViT-B/16] (main.py 114): INFO Max accuracy: 8.68%
[2025-03-10 01:50:32 ViT-B/16] (main.py 184): INFO Train: [8/11][0/296]	eta 0:15:11 lr 0.007248202	time 3.0789 (3.0789)	tot_loss 3.5572 (3.5572)	mem 6508MB
[2025-03-10 01:52:12 ViT-B/16] (main.py 184): INFO Train: [8/11][50/296]	eta 0:08:18 lr 0.006541045	time 2.0055 (2.0247)	tot_loss 3.4507 (3.4408)	mem 6508MB
[2025-03-10 01:53:52 ViT-B/16] (main.py 184): INFO Train: [8/11][100/296]	eta 0:06:35 lr 0.005865672	time 2.0018 (2.0155)	tot_loss 3.1276 (3.4346)	mem 6508MB
[2025-03-10 01:55:33 ViT-B/16] (main.py 184): INFO Train: [8/11][150/296]	eta 0:04:53 lr 0.005223655	time 2.0106 (2.0126)	tot_loss 3.2667 (3.4498)	mem 6508MB
[2025-03-10 01:57:13 ViT-B/16] (main.py 184): INFO Train: [8/11][200/296]	eta 0:03:13 lr 0.004616486	time 2.0037 (2.0110)	tot_loss 3.6847 (3.4505)	mem 6508MB
[2025-03-10 01:58:53 ViT-B/16] (main.py 184): INFO Train: [8/11][250/296]	eta 0:01:32 lr 0.004045580	time 2.0084 (2.0100)	tot_loss 3.1909 (3.4492)	mem 6508MB
[2025-03-10 02:00:24 ViT-B/16] (main.py 191): INFO EPOCH 8 training takes 0:09:54
[2025-03-10 02:00:24 ViT-B/16] (main.py 258): INFO 1 views inference
[2025-03-10 02:09:54 ViT-B/16] (main.py 299): INFO Training
[2025-03-10 02:09:54 ViT-B/16] (main.py 300): INFO  * Loss: 8.4026
[2025-03-10 02:09:54 ViT-B/16] (main.py 301): INFO  * Mean Rank: 191.105, Median Rank: 35.000
[2025-03-10 02:09:54 ViT-B/16] (main.py 302): INFO  * Recall@1: 7.981, Recall@5: 22.593, Recall@10: 31.883, Recall@50: 55.828
[2025-03-10 02:09:54 ViT-B/16] (main.py 258): INFO 1 views inference
