{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff034b9-55b2-4d3b-b4a7-3a9c2f7225aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code for performing inference with ViFi-CLIP on custom videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244065e2-c4be-4081-a343-ecd7ac15f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install in environment\n",
    "\"\"\"\n",
    "!pip install torch==1.11.0 -q\n",
    "!pip install torchvision==0.12.0 -q\n",
    "!pip install yacs ftfy timm regex -q\n",
    "!pip install mmcv-full -q\n",
    "!pip install decord\n",
    "\"\"\"\n",
    "# depending on possible errors\n",
    "#!pip install mmcv -q\n",
    "#!pip install mmengine -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9b8f3c-30d1-4c01-8b47-98827069dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings, logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from utils.config import get_config\n",
    "from utils.logger import create_logger\n",
    "from trainers import vificlip\n",
    "from datasets.pipeline import *\n",
    "from collections import OrderedDict\n",
    "#from mmcv import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1b0441-e28b-47b7-aad8-44732bb11131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949014f3-261a-4665-af64-39ba9b8229ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up config and logger\n",
    "def setup_config(config):\n",
    "    class parse_option():\n",
    "        def __init__(self):\n",
    "            self.config = config\n",
    "            self.output =  \"exp\" \n",
    "            self.resume = pretrained_model_path\n",
    "            self.only_test = True\n",
    "            self.opts = None\n",
    "            self.batch_size = None\n",
    "            self.pretrained = None\n",
    "            self.accumulation_steps = None\n",
    "            self.local_rank = 0\n",
    "\n",
    "    args = parse_option()\n",
    "    config = get_config(args)\n",
    "    logger = create_logger(output_dir=args.output, name=f\"{config.MODEL.ARCH}\")\n",
    "    \n",
    "    return config, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eafa669-70a2-4652-bf34-479cc2123194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess pipeline for the video frames\n",
    "def get_pipeline(config):\n",
    "    img_norm_cfg = dict(\n",
    "        mean=[123.675, 116.28, 103.53], \n",
    "        std=[58.395, 57.12, 57.375], \n",
    "        to_bgr=False\n",
    "    )\n",
    "\n",
    "    scale_resize = int(256 / 224 * config.DATA.INPUT_SIZE)\n",
    "\n",
    "    val_pipeline = [\n",
    "        dict(type='DecordInit'),\n",
    "        dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=config.DATA.NUM_FRAMES, test_mode=True),\n",
    "        dict(type='DecordDecode'),\n",
    "        dict(type='Resize', scale=(-1, scale_resize)),\n",
    "        dict(type='CenterCrop', crop_size=config.DATA.INPUT_SIZE),\n",
    "        dict(type='Normalize', **img_norm_cfg),\n",
    "        dict(type='FormatShape', input_format='NCHW'),\n",
    "        dict(type='Collect', keys=['imgs'], meta_keys=[]),\n",
    "        dict(type='ToTensor', keys=['imgs'])\n",
    "    ]\n",
    "\n",
    "    if config.TEST.NUM_CROP == 3:\n",
    "        val_pipeline[3] = dict(type='Resize', scale=(-1, config.DATA.INPUT_SIZE))\n",
    "        val_pipeline[4] = dict(type='ThreeCrop', crop_size=config.DATA.INPUT_SIZE)\n",
    "\n",
    "    if config.TEST.NUM_CLIP > 1:\n",
    "        val_pipeline[1] = dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=config.DATA.NUM_FRAMES, multiview=config.TEST.NUM_CLIP)\n",
    "\n",
    "    pipeline = Compose(val_pipeline)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7c07c6-08f0-424a-98eb-e1428dcedd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict similarity score for video + instruction\n",
    "def predict(video_path, instruction):\n",
    "    model = vificlip.returnCLIP(\n",
    "        config,\n",
    "        logger=logger,\n",
    "        class_names=instruction,\n",
    "    )\n",
    "\n",
    "    model = model.float().cuda()\n",
    "\n",
    "    dict_file = {'filename': video_path, \n",
    "                 'tar': False, \n",
    "                 'modality': 'RGB', \n",
    "                 'start_index': 0}\n",
    "    \n",
    "    video = pipeline(dict_file)\n",
    "    video_tensor = video['imgs'].unsqueeze(0).cuda().float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(video_tensor)\n",
    "\n",
    "    if len(instruction)>1: # for multiple instrutions given \n",
    "        return logits[0].tolist()\n",
    "    else: # single instruction\n",
    "        return logits[0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3efef3d-0878-4498-bbcf-1be87c12bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation (spearman, person) scores for columns of two datafranes\n",
    "def get_correlation_df(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2, on=['turn', 'id'], suffixes=('_df1', '_df2'))\n",
    "\n",
    "    correlation_results = []\n",
    "\n",
    "    for df1_col in df1.columns:\n",
    "        if df1_col in ['turn', 'id']:\n",
    "            continue \n",
    "\n",
    "        for df2_col in df2.columns:\n",
    "            if df2_col in ['turn', 'id']:\n",
    "                continue \n",
    "            \n",
    "            spearman_corr, spearman_p_value = stats.spearmanr(merged_df[df1_col], merged_df[df2_col])\n",
    "            pearson_corr, pearson_p_value = stats.pearsonr(merged_df[df1_col], merged_df[df2_col])\n",
    "\n",
    "            correlation_results.append({\n",
    "                'df1': df1_col,\n",
    "                'df2': df2_col,\n",
    "                'spearman_corr': spearman_corr, \n",
    "                'spearman_p_value': spearman_p_value,\n",
    "                'pearson_corr': pearson_corr, \n",
    "                'pearson_p_value': pearson_p_value\n",
    "            })\n",
    "\n",
    "    correlation_results_df = pd.DataFrame(correlation_results)\n",
    "    return correlation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1576ec4-2054-4f43-b74e-d9720ef5dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read required data (huamn judgement, edit turns, data), analyse and save in df\n",
    "def compute_correlation(save_dir, save_pred_scores=False, save_pred_path=None):\n",
    "    if save_pred_scores:\n",
    "        assert save_pred_path\n",
    "        \n",
    "    samples = pd.read_csv(\"/home/jovyan/BA/Github/thesis-edit-evaluation/open_clip/src/open_clip_inference/human_scores.csv\", sep=\";\")\n",
    "    pattern = r'(\\d+)-output(\\d+)'\n",
    "    \n",
    "    with open('/home/jovyan/BA/Github/thesis-edit-evaluation/open_clip/src/open_clip_inference/edit_turns.json') as f:\n",
    "        turns = json.load(f)\n",
    "\n",
    "    dev = pd.read_csv(\"/home/jovyan/BA/Github/MagicBrush/dev_data.csv\", sep=\",\")\n",
    "\n",
    "    results = []\n",
    "    for index, row in tqdm(samples.iterrows(), \"Predicting...\"):\n",
    "        current_id = int(row[\"id\"])\n",
    "        current_turn = int(row[\"turn\"])\n",
    "\n",
    "        for entry in turns:\n",
    "            output = entry[\"output\"]\n",
    "            match = re.search(pattern, output)\n",
    "\n",
    "            if match:\n",
    "                found_id = int(match.group(1)) # get id of sample\n",
    "                found_turn = int(match.group(2)) # get turn of sample\n",
    "\n",
    "                if int(found_id) == current_id and int(found_turn) == current_turn:\n",
    "                    instruction = entry[\"instruction\"].lower()\n",
    "                    \n",
    "                    frames = config.DATA.NUM_FRAMES\n",
    "                    magicbrush_dir = \"/home/jovyan/BA/Github/MagicBrush\"\n",
    "                    \n",
    "                    if frames==2:\n",
    "                        video_path = f\"{magicbrush_dir}/vifi_format/videos/{current_id}_{current_turn}.mp4\"\n",
    "                    elif frames==8:\n",
    "                        video_path = f\"{magicbrush_dir}/videos_8_frames/{current_id}_{current_turn}.mp4\"\n",
    "\n",
    "                    similarity = predict(video_path, instruction)\n",
    "\n",
    "                    row = {\n",
    "                        \"id\": current_id,\n",
    "                        \"turn\": current_turn,\n",
    "                        \"vificlip_score\": similarity,\n",
    "                    }\n",
    "                    results.append(row)\n",
    "\n",
    "    vifi_scores = pd.DataFrame(results)\n",
    "    if save_pred_scores:\n",
    "        vifi_scores.to_csv(save_pred_path, index=False)\n",
    "\n",
    "    #vifi_scores = vifi_scores.drop_duplicates()\n",
    "    correlation_df = get_correlation_df(samples, vifi_scores)\n",
    "    correlation_df.to_csv(f\"{save_dir}\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919c8171-dc0f-4dfb-a446-23ae511d433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from output/base2novel/humanedit/vitb32_2_80_20/16_32_vifi_clip_all_shot.yaml\n"
     ]
    }
   ],
   "source": [
    "config = \"output/base2novel/humanedit/vitb32_2_80_20/16_32_vifi_clip_all_shot.yaml\"\n",
    "pretrained_model_path = \"output/base2novel/humanedit/vitb32_2_80_20/ckpt_epoch_10.pth\"\n",
    "\n",
    "config, logger = setup_config(config)\n",
    "pipeline = get_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "156f15f2-5695-470a-a0fc-0777876739a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 20.421875\n"
     ]
    }
   ],
   "source": [
    "# option 1: perform single inference\n",
    "\n",
    "video_path = \"/home/jovyan/BA/Github/MagicBrush/vifi_format/videos/100081_1.mp4\"\n",
    "instruction = [\"remove the dog\"]\n",
    "similarity = predict(video_path, instruction)\n",
    "print(f\"Similarity score: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcec2d29-5be5-4ad6-8707-b560d11a4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting...: 77it [02:14,  1.74s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# option 2: compute correlation between human scores and vifi-clip's predictions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcompute_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorrelation/base2novel/humanedit/test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mcompute_correlation\u001b[0;34m(save_dir, save_pred_scores, save_pred_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m     vifi_scores\u001b[38;5;241m.\u001b[39mto_csv(save_pred_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#vifi_scores = vifi_scores.drop_duplicates()\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m correlation_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_correlation_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvifi_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m correlation_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mget_correlation_df\u001b[0;34m(df1, df2)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df2_col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \n\u001b[0;32m---> 15\u001b[0m spearman_corr, spearman_p_value \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspearmanr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf1_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf2_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m pearson_corr, pearson_p_value \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mpearsonr(merged_df[df1_col], merged_df[df2_col])\n\u001b[1;32m     18\u001b[0m correlation_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf1\u001b[39m\u001b[38;5;124m'\u001b[39m: df1_col,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf2\u001b[39m\u001b[38;5;124m'\u001b[39m: df2_col,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson_p_value\u001b[39m\u001b[38;5;124m'\u001b[39m: pearson_p_value\n\u001b[1;32m     25\u001b[0m })\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4526\u001b[0m, in \u001b[0;36mspearmanr\u001b[0;34m(a, b, axis, nan_policy, alternative)\u001b[0m\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SpearmanrResult(np\u001b[38;5;241m.\u001b[39mnan, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m   4525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axisout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (a[:, \u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m a[:, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m():\n\u001b[1;32m   4527\u001b[0m         \u001b[38;5;66;03m# If an input is constant, the correlation coefficient\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;66;03m# is not defined.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(SpearmanRConstantInputWarning())\n\u001b[1;32m   4530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m SpearmanrResult(np\u001b[38;5;241m.\u001b[39mnan, np\u001b[38;5;241m.\u001b[39mnan)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "# option 2: compute correlation between human scores and vifi-clip's predictions\n",
    "compute_correlation(save_dir=\"correlation/base2novel/humanedit/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb7bee-d36e-4899-b1fe-9c109661ace0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (viFi-CLIP)",
   "language": "python",
   "name": "vifi-clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
