{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fea83d3-1964-47cb-97ec-d0df9210cb64",
   "metadata": {},
   "source": [
    "# GPT-4o-as-a-judge\n",
    "\n",
    "Initial experiments using GPT4o for data labeling. Eventually not included in the thesis report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae592c-8f4b-4b48-8539-a1e7d3f0dd3a",
   "metadata": {},
   "source": [
    "[Prompt Instruction](https://platform.openai.com/docs/guides/vision) with visual inputs (OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0356230-765f-4c3a-aaa1-2e82ca7cd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python -q\n",
    "!pip install torchvision -q\n",
    "!pip install torchmetrics -q\n",
    "!pip install torchmetrics[image] -q\n",
    "!pip install \"torchmetrics[image]\" -q\n",
    "!pip install torch-fidelity -q\n",
    "!pip install numpy -q\n",
    "!pip install torchmetrics -q\n",
    "!pip install torch -q\n",
    "!pip install openai -q\n",
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e050ddfb-cf6e-4711-83aa-6037de5c1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import base64\n",
    "import requests\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23f6d80-7479-461b-84f4-f61197aa0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "#load_dotenv(dotenv_path=\"/home/jovyan/BA/Github/thesis-edit-evaluation/.env\")\n",
    "\n",
    "MODEL = \"gpt-4o-2024-08-06\"\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2874975-9839-4f0c-a0b8-ff07e039bfa0",
   "metadata": {},
   "source": [
    "Define prompts for evaluation, similar like user study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feebf359-6159-401c-8b96-35e3c441a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You want to change the content of a specific area within an image. This technique is called text-guided image editing. \n",
    "\n",
    "It involves the following elements:\n",
    "- Original Image: The starting image before any edits.\n",
    "- Prompt: A text description that specifies the desired change in the original image.\n",
    "- Mask: A defined area in the image where the change should occur according to the prompt.\n",
    "- Edited Image: The final image after the desired change has been made.\n",
    "\n",
    "Your task is to evaluate the quality of these edits by considering three different aspects. \n",
    "Each aspect should be rated on a scale from 1 to 10, where 1 indicates \"very poor\" and 10 means \"excellent.\"\n",
    "\n",
    "Aspects to Evaluate:\n",
    "1. Prompt-Image Alignment: \n",
    "    - Objective: Assess how well the edited area aligns with the instructions provided in the text prompt.\n",
    "    - Considerations: Verify if the desired changes are accurately implemented. Pay attention to details such as numbers, colors, and objects mentioned in the prompt.\n",
    "2. Visual Quality: \n",
    "    - Objective: Evaluate the visual appeal of the edited area within the mask, focusing solely on the appearance of the new content within the masked area.\n",
    "    - Considerations: Assess realism and aesthetics, including color accuracy and overall visual coherence.\n",
    "3. Consistency Between Original Image and Edited Area: \n",
    "    - Objective: Measure how well the edit integrates with the original image.\n",
    "    - Considerations: Examine consistency in style, lighting, logic, and spatial coherence between the edited area and the original image.\n",
    "4. Overall Rating:\n",
    "    - After evaluating each aspect individually, provide an overall rating of the entire edited image. Consider how you perceive and like the edit as a whole, how well it meets your expectations and integrates with the original image. \n",
    "\n",
    "\n",
    "Input and Output:\n",
    "- Input: The evaluation will be based on the following items:\n",
    "    - Original image\n",
    "    - Text prompt\n",
    "    - Image with a masked area\n",
    "    - Edited image\n",
    "- Output: Provide your ratings in the following JSON format. Fill \"score\" keys with numerical values.\n",
    "{\n",
    "  \"alignment\": \"\",\n",
    "  \"visual_quality\": \"\",\n",
    "  \"consistency\": \"\",\n",
    "  \"overall\": \"\"\n",
    "}\n",
    "\n",
    "Additional Instructions:\n",
    "- Careful justification: Think carefully about your ratings and. Avoid providing ratings without thoughtful consideration.\n",
    "- Output: Do not include anything other than the JSON file in your response.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_user_prompt(instruction):\n",
    "    return f\"Evaluate the quality given the following prompt: {instruction}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b14404-c74d-4172-b41d-fcb79dded911",
   "metadata": {},
   "source": [
    "Additionally to prompt, input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c091b2-e200-4034-9b4f-5ce637c3feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def call_api(prompt, img_original, img_mask, img_edited, instruction):\n",
    "    response = client.chat.completions.create(\n",
    "        model= MODEL, #\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\", \n",
    "                        \"text\": get_user_prompt(instruction)\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\", \n",
    "                        \"image_url\": {\n",
    "                            \"url\": f'data:image/png;base64,{img_original}'\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_mask}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_edited}\"\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "751017a2-030f-4fcd-87c4-74d59a7667f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['model', 'id', 'turn', 'alignment', 'visual_quality', 'consistency', 'overall']\n",
    "df_gpt = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d56aaf4-ec8e-48f1-9037-be3b1a4a40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image):\n",
    "    if isinstance(image, Image.Image):\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")  # oder ein anderes unterst√ºtztes Format\n",
    "        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a PIL Image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5e6e2-c3a7-4f3f-9207-54f396604bfd",
   "metadata": {},
   "source": [
    "GPT to judge all samples within the dev split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24d9bd74-715e-4922-8fcb-f80d15fc63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_split = pd.read_csv(\"/home/jovyan/BA/Github/MagicBrush/dev_data_with_mask.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5896abf4-a129-49be-9956-c79ba2caaad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 1it [00:03,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alignment': '8', 'visual_quality': '8', 'consistency': '9', 'overall': '8'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = r'(\\d+)-output(\\d+)'\n",
    "target_size = (512,512)\n",
    "\n",
    "for index, row in tqdm(dev_split.iterrows(), desc=\"Progress\"):\n",
    "    id = row[\"img_id\"]\n",
    "    turn = row[\"turn_index\"]\n",
    "    instruction = row[\"instruction\"]\n",
    "\n",
    "    # open images, resize to 512x512, and convert to RGB\n",
    "    output_image = Image.open(row[\"target_img\"]).resize(target_size).convert('RGB')\n",
    "    input_image = Image.open(row[\"source_img\"]).resize(target_size).convert('RGB')\n",
    "    mask_image = Image.open(row[\"mask_img\"]).resize(target_size).convert('RGB')\n",
    "\n",
    "    output_array = np.array(output_image)\n",
    "    mask_array = np.array(mask_image)\n",
    "    masked_area = cv2.absdiff(output_array, mask_array)\n",
    "\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    masked_area_tensor = to_tensor(masked_area).unsqueeze(0)\n",
    "    masked_area_image = transforms.ToPILImage()(masked_area_tensor.squeeze(0))\n",
    "\n",
    "    input_image_encoded = encode_image(input_image)\n",
    "    masked_area_encoded = encode_image(masked_area_image)\n",
    "    output_image_encoded = encode_image(output_image)\n",
    "\n",
    "    response = call_api(\n",
    "        API_KEY,\n",
    "        input_image_encoded,\n",
    "        masked_area_encoded, \n",
    "        output_image_encoded,\n",
    "        instruction\n",
    "    )\n",
    "\n",
    "    response = json.loads(response)\n",
    "    print(response)\n",
    "\n",
    "    new_row = pd.DataFrame({\n",
    "        \"model\": [MODEL],\n",
    "        \"id\": [id],\n",
    "        \"turn\": [turn], \n",
    "        \"alignment\": [response.get(\"alignment\", None)],\n",
    "        \"visual_quality\": [response.get(\"visual_quality\", None)],\n",
    "        \"consistency\": [response.get(\"consistency\", None)], \n",
    "        \"overall\": [response.get(\"overall\", None)]\n",
    "    })\n",
    "\n",
    "    df_gpt = pd.concat([df_gpt, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c71ac449-d2b2-4f5e-93c1-0d106e25c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt.to_csv(\"gpt_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
