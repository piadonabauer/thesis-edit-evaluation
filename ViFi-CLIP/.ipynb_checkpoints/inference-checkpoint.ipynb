{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff034b9-55b2-4d3b-b4a7-3a9c2f7225aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code for performing inference with ViFi-CLIP on custom videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244065e2-c4be-4081-a343-ecd7ac15f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install in environment\n",
    "\"\"\"\n",
    "!pip install torch==1.11.0 -q\n",
    "!pip install torchvision==0.12.0 -q\n",
    "!pip install yacs ftfy timm regex -q\n",
    "!pip install mmcv-full -q\n",
    "!pip install decord\n",
    "\"\"\"\n",
    "# depending on possible errors\n",
    "#!pip install mmcv -q\n",
    "#!pip install mmengine -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f9b8f3c-30d1-4c01-8b47-98827069dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from utils.config import get_config\n",
    "from utils.logger import create_logger\n",
    "from trainers import vificlip\n",
    "from datasets.pipeline import *\n",
    "from collections import OrderedDict\n",
    "#from mmcv import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1b0441-e28b-47b7-aad8-44732bb11131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "949014f3-261a-4665-af64-39ba9b8229ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up config and logger\n",
    "def setup_config(config):\n",
    "    class parse_option():\n",
    "        def __init__(self):\n",
    "            self.config = config\n",
    "            self.output =  \"exp\" \n",
    "            self.resume = pretrained_model_path\n",
    "            self.only_test = True\n",
    "            self.opts = None\n",
    "            self.batch_size = None\n",
    "            self.pretrained = None\n",
    "            self.accumulation_steps = None\n",
    "            self.local_rank = 0\n",
    "\n",
    "    args = parse_option()\n",
    "    config = get_config(args)\n",
    "    logger = create_logger(output_dir=args.output, name=f\"{config.MODEL.ARCH}\")\n",
    "    \n",
    "    return config, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eafa669-70a2-4652-bf34-479cc2123194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess pipeline for the video frames\n",
    "def get_pipeline(config):\n",
    "    img_norm_cfg = dict(\n",
    "        mean=[123.675, 116.28, 103.53], \n",
    "        std=[58.395, 57.12, 57.375], \n",
    "        to_bgr=False\n",
    "    )\n",
    "\n",
    "    scale_resize = int(256 / 224 * config.DATA.INPUT_SIZE)\n",
    "\n",
    "    val_pipeline = [\n",
    "        dict(type='DecordInit'),\n",
    "        dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=config.DATA.NUM_FRAMES, test_mode=True),\n",
    "        dict(type='DecordDecode'),\n",
    "        dict(type='Resize', scale=(-1, scale_resize)),\n",
    "        dict(type='CenterCrop', crop_size=config.DATA.INPUT_SIZE),\n",
    "        dict(type='Normalize', **img_norm_cfg),\n",
    "        dict(type='FormatShape', input_format='NCHW'),\n",
    "        dict(type='Collect', keys=['imgs'], meta_keys=[]),\n",
    "        dict(type='ToTensor', keys=['imgs'])\n",
    "    ]\n",
    "\n",
    "    if config.TEST.NUM_CROP == 3:\n",
    "        val_pipeline[3] = dict(type='Resize', scale=(-1, config.DATA.INPUT_SIZE))\n",
    "        val_pipeline[4] = dict(type='ThreeCrop', crop_size=config.DATA.INPUT_SIZE)\n",
    "\n",
    "    if config.TEST.NUM_CLIP > 1:\n",
    "        val_pipeline[1] = dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=config.DATA.NUM_FRAMES, multiview=config.TEST.NUM_CLIP)\n",
    "\n",
    "    pipeline = Compose(val_pipeline)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c7c07c6-08f0-424a-98eb-e1428dcedd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict similarity score for video + instruction\n",
    "def predict(video_path, instruction):\n",
    "    model = vificlip.returnCLIP(\n",
    "        config,\n",
    "        logger=logger,\n",
    "        class_names=instruction,\n",
    "    )\n",
    "\n",
    "    model = model.float().cuda()\n",
    "\n",
    "    dict_file = {'filename': video_path, \n",
    "                 'tar': False, \n",
    "                 'modality': 'RGB', \n",
    "                 'start_index': 0}\n",
    "    \n",
    "    video = pipeline(dict_file)\n",
    "    video_tensor = video['imgs'].unsqueeze(0).cuda().float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(video_tensor)\n",
    "\n",
    "    if len(instruction)>1: # for multiple instrutions given \n",
    "        return logits[0].tolist()\n",
    "    else: # single instruction\n",
    "        return logits[0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3efef3d-0878-4498-bbcf-1be87c12bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation (spearman, person) scores for columns of two datafranes\n",
    "def get_correlation_df(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2, on=['turn', 'id'], suffixes=('_df1', '_df2'))\n",
    "\n",
    "    correlation_results = []\n",
    "\n",
    "    for df1_col in df1.columns:\n",
    "        if df1_col in ['turn', 'id']:\n",
    "            continue \n",
    "\n",
    "        for df2_col in df2.columns:\n",
    "            if df2_col in ['turn', 'id']:\n",
    "                continue \n",
    "            \n",
    "            spearman_corr, spearman_p_value = stats.spearmanr(merged_df[df1_col], merged_df[df2_col])\n",
    "            pearson_corr, pearson_p_value = stats.pearsonr(merged_df[df1_col], merged_df[df2_col])\n",
    "\n",
    "            correlation_results.append({\n",
    "                'df1': df1_col,\n",
    "                'df2': df2_col,\n",
    "                'spearman_corr': spearman_corr, \n",
    "                'spearman_p_value': spearman_p_value,\n",
    "                'pearson_corr': pearson_corr, \n",
    "                'pearson_p_value': pearson_p_value\n",
    "            })\n",
    "\n",
    "    correlation_results_df = pd.DataFrame(correlation_results)\n",
    "    return correlation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1576ec4-2054-4f43-b74e-d9720ef5dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read required data (huamn judgement, edit turns, data), analyse and save in df\n",
    "def compute_correlation(save_dir, save_pred_scores=False, save_pred_path=None):\n",
    "    if save_pred_scores:\n",
    "        assert save_pred_path\n",
    "        \n",
    "    samples = pd.read_csv(\"/home/jovyan/BA/Github/thesis-edit-evaluation/open_clip/src/open_clip_inference/human_scores.csv\", sep=\";\")\n",
    "    pattern = r'(\\d+)-output(\\d+)'\n",
    "    \n",
    "    with open('/home/jovyan/BA/Github/thesis-edit-evaluation/open_clip/src/open_clip_inference/edit_turns.json') as f:\n",
    "        turns = json.load(f)\n",
    "\n",
    "    dev = pd.read_csv(\"/home/jovyan/BA/Github/MagicBrush/dev_data.csv\", sep=\",\")\n",
    "\n",
    "    results = []\n",
    "    for index, row in tqdm(samples.iterrows(), \"Predicting...\"):\n",
    "        current_id = int(row[\"id\"])\n",
    "        current_turn = int(row[\"turn\"])\n",
    "\n",
    "        for entry in turns:\n",
    "            output = entry[\"output\"]\n",
    "            match = re.search(pattern, output)\n",
    "\n",
    "            if match:\n",
    "                found_id = int(match.group(1)) # get id of sample\n",
    "                found_turn = int(match.group(2)) # get turn of sample\n",
    "\n",
    "                if int(found_id) == current_id and int(found_turn) == current_turn:\n",
    "                    instruction = entry[\"instruction\"].lower()\n",
    "                    \n",
    "                    frames = config.DATA.NUM_FRAMES\n",
    "                    magicbrush_dir = \"/home/jovyan/BA/Github/MagicBrush\"\n",
    "                    \n",
    "                    if frames==2:\n",
    "                        video_path = f\"{magicbrush_dir}/vifi_format/videos/{current_id}_{current_turn}.mp4\"\n",
    "                    elif frames==8:\n",
    "                        video_path = f\"{magicbrush_dir}/videos_8_frames/{current_id}_{current_turn}.mp4\"\n",
    "\n",
    "                    similarity = predict(video_path, instruction)\n",
    "\n",
    "                    row = {\n",
    "                        \"id\": current_id,\n",
    "                        \"turn\": current_turn,\n",
    "                        \"vificlip_score\": similarity,\n",
    "                    }\n",
    "                    results.append(row)\n",
    "\n",
    "    vifi_scores = pd.DataFrame(results)\n",
    "    if save_pred_scores:\n",
    "        vifi_scores.to_csv(save_pred_path, index=False)\n",
    "\n",
    "    #vifi_scores = vifi_scores.drop_duplicates()\n",
    "    correlation_df = get_correlation_df(samples, vifi_scores)\n",
    "    correlation_df.to_csv(f\"{save_dir}\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "919c8171-dc0f-4dfb-a446-23ae511d433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from output/base2novel/humanedit/vitb32_2_80_20/16_32_vifi_clip_all_shot.yaml\n"
     ]
    }
   ],
   "source": [
    "config = \"output/base2novel/humanedit/vitb32_2_80_20/16_32_vifi_clip_all_shot.yaml\"\n",
    "pretrained_model_path = \"output/base2novel/humanedit/vitb32_2_80_20/ckpt_epoch_10.pth\"\n",
    "\n",
    "config, logger = setup_config(config)\n",
    "pipeline = get_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "156f15f2-5695-470a-a0fc-0777876739a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 20.421875\n"
     ]
    }
   ],
   "source": [
    "# option 1: perform single inference\n",
    "\n",
    "video_path = \"/home/jovyan/BA/Github/MagicBrush/vifi_format/videos/100081_1.mp4\"\n",
    "instruction = [\"remove the dog\"]\n",
    "similarity = predict(video_path, instruction)\n",
    "print(f\"Similarity score: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec2d29-5be5-4ad6-8707-b560d11a4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: compute correlation between human scores and vifi-clip's predictions\n",
    "compute_correlation(save_dir=\"correlation/base2novel/humanedit/vitb32_2_10_epochs_80_20.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb7bee-d36e-4899-b1fe-9c109661ace0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (viFi-CLIP)",
   "language": "python",
   "name": "vifi-clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
