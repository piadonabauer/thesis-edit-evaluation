{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02f4af0-3a40-4bda-a30c-c7acfe1077d2",
   "metadata": {},
   "source": [
    "# Custom Datasets\n",
    "\n",
    "- [HuggingFace MagicBrush](https://huggingface.co/datasets/osunlp/MagicBrush)\n",
    "- [HuggingFace AURORA](https://huggingface.co/datasets/McGill-NLP/AURORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94f33fb-613c-4be8-863c-b1acf18de2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-18.1.0\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (4.64.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9653e14-9f6e-48e1-bfb9-66375f071b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import time\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641e7bba-adae-431c-9f19-c16f99f05655",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc615475-8465-47d4-9ff9-99c264631cca",
   "metadata": {},
   "source": [
    "### How to get data\n",
    "\n",
    "- Clone repository. e.g. MagicBrush: 'git clone https://huggingface.co/datasets/osunlp/MagicBrush'\n",
    "    - Get files in parquet format (10+ files)\n",
    "- Use code below to save images from parquet in folders, one folder for each parquet\n",
    "- Columns of df containing the path to the images: img_id, turn_index, source_img (str path), target_img (str path), instruction\n",
    "    - one df for train data (8,807)\n",
    "    - one df for dev data (528)\n",
    "- For training: use path in df as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d574070-99c4-4f1b-b6df-b460973e086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of parquet files\n",
    "parquet_dir = '/home/jovyan/BA/Github/MagicBrush/data/'\n",
    "#\"/home/jovyan/BA/Github/AURORA/data\" # AURORA\n",
    "#'/home/jovyan/BA/Github/MagicBrush/data/' # MagicBrush\n",
    "\n",
    "# path to save the images and dataframes\n",
    "base_image_dir = '/home/jovyan/BA/Github/MagicBrush/'\n",
    "#'/home/jovyan/BA/Github/AURORA/'\n",
    "#'/home/jovyan/BA/Github/MagicBrush/'\n",
    "\n",
    "os.makedirs(base_image_dir, exist_ok=True)\n",
    "\n",
    "# init dataframes\n",
    "#train_df = pd.DataFrame(columns=['source_img', 'target_img', 'mask_img', 'instruction'])\n",
    "#dev_df = pd.DataFrame(columns=['source_img', 'target_img',  'mask_img', 'instruction'])\n",
    "\n",
    "train_df = pd.DataFrame(columns=['mask_img', 'instruction'])\n",
    "dev_df = pd.DataFrame(columns=['mask_img', 'instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4597f60-ad47-4d57-a36d-5ce77493b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_image_name(base_name, idx):\n",
    "    unique_str = f\"{base_name}_{idx}_{int(time.time()*1000)}\"\n",
    "    return hashlib.md5(unique_str.encode()).hexdigest() + \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cccc8dbe-345c-4d2c-a77b-0cb282d5f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, save_dir, save=False): # AURORA: add input_name\n",
    "    image_data = image['bytes']\n",
    "    img = Image.open(io.BytesIO(image_data))\n",
    "    img = img.resize((224, 224))\n",
    "    \n",
    "    save_path = os.path.join(save_dir, image['path']) #AURORA: only image\n",
    "    #img.save(save_path)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee507f39-f770-4d86-a843-2b52fa871c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    # AURORA\\n    new_row = {\\n        'input': source_img_path,\\n        'output': target_img_path,\\n        'instruction': row['instruction']\\n    }\\n    return new_row\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_new_row(idx, row, parquet_image_dir):\n",
    "    # MagicBrush\n",
    "    #source_path = process_image(row['source_img'], parquet_image_dir)\n",
    "    #target_path = process_image(row['target_img'], parquet_image_dir)\n",
    "    mask_path = process_image(row['mask_img'], parquet_image_dir, save=True)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # AURORA\n",
    "    unique_input_name = generate_unique_image_name(\"input\", idx)\n",
    "    unique_output_name = generate_unique_image_name(\"output\", idx)\n",
    "\n",
    "    source_path = process_image(row['input'], parquet_image_dir, unique_input_name)\n",
    "    target_path = process_image(row['output'], parquet_image_dir, unique_output_name)\n",
    "\n",
    "    source_img_path = source_path\n",
    "    target_img_path = target_path\n",
    "    \"\"\"\n",
    "\n",
    "# Create new row with required information (MagicBrush)\n",
    "    new_row = {\n",
    "        'img_id': row['img_id'],\n",
    "        'turn_index': row['turn_index'],\n",
    "        #'source_img': source_path,\n",
    "        #'target_img': target_path,\n",
    "        'mask_img': mask_path,\n",
    "        'instruction': row['instruction']\n",
    "    }\n",
    "    return new_row\n",
    "\n",
    "\"\"\"\n",
    "    # AURORA\n",
    "    new_row = {\n",
    "        'input': source_img_path,\n",
    "        'output': target_img_path,\n",
    "        'instruction': row['instruction']\n",
    "    }\n",
    "    return new_row\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196eb8e-ee80-488f-9910-a6b08410ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.16it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.96it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.40it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.21it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.84it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.67it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.41it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.54it/s]\n",
      "Processing rows: 100%|██████████| 132/132 [00:02<00:00, 45.29it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.12it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.88it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.82it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.45it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.22it/s]\n",
      "Processing rows: 100%|██████████| 132/132 [00:02<00:00, 44.51it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.79it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.42it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.03it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.51it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.25it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.04it/s]\n",
      "Processing rows: 100%|██████████| 132/132 [00:02<00:00, 44.36it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.07it/s]\n",
      "Processing rows: 100%|██████████| 132/132 [00:02<00:00, 44.80it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 45.52it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.07it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.33it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.71it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.36it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.71it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.05it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.95it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.09it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 45.00it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.03it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.87it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 45.13it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.35it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.74it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 43.91it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.17it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.42it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.56it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.61it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.42it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 43.86it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.45it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 45.24it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.56it/s]\n",
      "Processing rows: 100%|██████████| 173/173 [00:03<00:00, 44.76it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.75it/s]\n",
      "Processing rows: 100%|██████████| 172/172 [00:03<00:00, 44.93it/s]\n",
      "Processing rows:  88%|████████▊ | 153/173 [00:03<00:00, 44.35it/s]"
     ]
    }
   ],
   "source": [
    "# loop over all parquet files\n",
    "\n",
    "for filename in os.listdir(parquet_dir):\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(parquet_dir, filename)\n",
    "        temp_df = pd.read_parquet(file_path)\n",
    "        \n",
    "        parquet_image_dir = os.path.join(base_image_dir, os.path.splitext(filename)[0])\n",
    "        os.makedirs(parquet_image_dir, exist_ok=True)\n",
    "        \n",
    "        \"\"\" Only for MagicBrush: filter out mask images, as we do not need them\n",
    "        if 'mask_img' in temp_df.columns: \n",
    "            temp_df = temp_df.drop(columns=['mask_img'])\n",
    "        \"\"\"\n",
    "\n",
    "        # MagicBrush\n",
    "        temp_processed_df = pd.DataFrame(columns=['img_id', 'turn_index', 'mask_img','instruction'])\n",
    "        \n",
    "        # AURORA\n",
    "        #temp_processed_df = pd.DataFrame(columns=['input', 'output', 'instruction'])\n",
    "        \n",
    "        processed_rows = []\n",
    "        for idx, row in tqdm(temp_df.iterrows(), total=len(temp_df), desc=\"Processing rows\"):\n",
    "            new_row = get_new_row(idx, row, parquet_image_dir)\n",
    "            processed_rows.append(new_row)\n",
    "        temp_processed_df = pd.concat([temp_processed_df, pd.DataFrame(processed_rows)], ignore_index=True)\n",
    "\n",
    "        if filename.startswith(\"train\"):\n",
    "            train_df = pd.concat([train_df, temp_processed_df])\n",
    "\n",
    "        if filename.startswith(\"dev\"):\n",
    "            dev_df = pd.concat([dev_df, temp_processed_df])\n",
    "\n",
    "train_df.to_csv(os.path.join(base_image_dir, 'train_data_mask_111.csv'), index=False)\n",
    "dev_df.to_csv(os.path.join(base_image_dir, 'dev_data_mask_111.csv'), index=False)\n",
    "\n",
    "print(\"Images processed and dataframes saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa7015-ebff-4f55-9850-f5cae11a928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799877ce-f4dc-4e2d-bb1c-f0397274ff74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
