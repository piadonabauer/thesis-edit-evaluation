{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Metrics\n",
    "Summary: https://pypi.org/project/piq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python -q\n",
    "!pip install torchvision -q\n",
    "!pip install torch-fidelity -q\n",
    "!pip install numpy -q\n",
    "!pip install torch -q\n",
    "!pip install 'transformers>=4.10.0' -q\n",
    "#!pip install https://github.com/Lightning-AI/torchmetrics/archive/master.zip -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DreamSIM\n",
    "!pip install dreamsim -q\n",
    "#!mkdir models/\n",
    "#!wget -O models/open_clip_vitb32_pretrain.pth.tar https://github.com/ssundaram21/dreamsim/releases/download/v0.1.0/open_clip_vitb32_pretrain.pth.tar -q\n",
    "from dreamsim import dreamsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install torchmetrics -q\n",
    "!python3 -m pip install torchmetrics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.regression import MeanAbsoluteError, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load manually sampled examples:\n",
    "- Good examples have a \"human_rating_binary\" of 1,\n",
    "- Bad examples of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv(\"./samples.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>turn</th>\n",
       "      <th>human_rating_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5434</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43425</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54492</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  turn  human_rating_binary\n",
       "0     49     2                    0\n",
       "1     49     3                    0\n",
       "2   5434     3                    0\n",
       "3  43425     2                    0\n",
       "4  54492     2                    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input image names and text instruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./edit_turns.json') as f:\n",
    "    turns = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': '142585-input.png',\n",
       "  'mask': '142585-mask1.png',\n",
       "  'output': '142585-output1.png',\n",
       "  'instruction': 'Let the van turn black.'},\n",
       " {'input': '392687-input.png',\n",
       "  'mask': '392687-mask1.png',\n",
       "  'output': '392687-output1.png',\n",
       "  'instruction': 'change the scooter into a skateboard'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turns[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip-Score\n",
    "- [Link](https://lightning.ai/docs/torchmetrics/stable/multimodal/clip_score.html)\n",
    "- Score is between 0 and 100, higher = better\n",
    "- Input\n",
    "    - img = single tensor with shape (N, C, H, W), or multiple tensors\n",
    "    - text = str, or tensor\n",
    "- Output\n",
    "    - float scalar tensor\n",
    "- Paras\n",
    "    - model_name_or_path, e.g. \"openai/clip-vit-base-patch16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc10daa39974b94b2f640313b00fb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344ba64f9afc4f31a2c6f8bd674b771e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6278d36eca84ad6915232f0dcdf3135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457e6ce5b464444799ad8c2fa78ba02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8fe0e80018421799d390fc13170f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280ffcbe4bbb486ead53812dee98f0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf1c810e44a472c89605686c4621b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e6e6dffcc74b0880de4922b3a127da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4647b8e89bff48e2b4924ffe82c4ca9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID\n",
    "[Link](https://lightning.ai/docs/torchmetrics/stable/image/frechet_inception_distance.html)\n",
    "- To measure the similarity (distribution) between two datasets of images\n",
    "- Requires multiple images for calculation!! Still incorporate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fid = FrechetInceptionDistance(feature=64)\n",
    "#fid.set_dtype(torch.float64) # for better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPIPS\n",
    "[Link](https://lightning.ai/docs/torchmetrics/stable/image/learned_perceptual_image_patch_similarity.html)\n",
    "- Calculates perceptual similarity between two images, low=more similar\n",
    "- Has been shown to match human perception\n",
    "\n",
    "Input\n",
    "- 2 tensors img, shape (N, 3, H, W). The minimum size of H, W depends on the chosen backbone\n",
    "\n",
    "Parameters\n",
    "- net_type (Literal['vgg', 'alex', 'squeeze'])\n",
    "- reduction (Literal['sum', 'mean']) – str indicating how to reduce over the batch dimension. Choose between ‘sum’ or ‘mean’.\n",
    "- normalize (bool) – by default this is False meaning that the input is expected to be in the [-1,1] range. If set to True will instead expect input to be in the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchmetrics/functional/image/lpips.py:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=False)\n"
     ]
    }
   ],
   "source": [
    "lpips = LearnedPerceptualImagePatchSimilarity(\n",
    "    net_type='vgg', \n",
    "    reduction='mean', \n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSIM\n",
    "\n",
    "[Link](https://lightning.ai/docs/torchmetrics/stable/image/structural_similarity.html)\n",
    "- Perceptual metric that quantifies image quality degradation\n",
    "- Between -1 and 1, where 1 indicates perfect similarity, 0 indicates no similarity, and -1 indicates perfect anti-correlation\n",
    "\n",
    "Output: \n",
    "- float scalar tensor with average SSIM value over sample else returns tensor of shape (N,) with SSIM values per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim = StructuralSimilarityIndexMeasure(\n",
    "    gaussian_kernel=True,\n",
    "    sigma=1.0,\n",
    "    reduction='elementwise_mean',\n",
    "    data_range=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSNR\n",
    "\n",
    "[Link](https://lightning.ai/docs/torchmetrics/stable/image/peak_signal_noise_ratio.html)\n",
    "- peak signal-to-noise ratio (PSNR)\n",
    "- Ratio between the maximum possible value (power) of a signal and the power of distorting noise that affects the quality of its representation\n",
    "\n",
    "Output\n",
    "- Range from 0 to infinity - Typically range from about 20 dB to 50 dB.\n",
    "- Lower values indicate poorer quality, while higher values indicate better quality. 0 dB indicates that the images are identical.\n",
    "\n",
    "Orientation\n",
    "- High Quality: Above 30 dB.\n",
    "- Medium Quality: Between 20 dB and 30 dB.\n",
    "- Low Quality: Below 20 dB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = PeakSignalNoiseRatio(\n",
    "    data_range=None,\n",
    "    base=10,\n",
    "    reduction='elementwise_mean',\n",
    "    dim=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE und MSE\n",
    "[Link to MAE](https://lightning.ai/docs/torchmetrics/stable/regression/mean_absolute_error.html)\n",
    "- Mean Absolute Error\n",
    "\n",
    "[Link to MSE](https://lightning.ai/docs/torchmetrics/stable/regression/mean_squared_error.html)\n",
    "- Mean Square Error\n",
    "- Compares the “true” pixel values of the original image to the degraded image\n",
    "- The MSE represents the average of the squares of the \"errors\" between the actual image and the noisy image.\n",
    "\n",
    "Output\n",
    "- Between 0 and infinity\n",
    "- A lower MAE value indicates that the edited image is closer to the original image, while a higher MAE indicates greater differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error = MeanAbsoluteError()\n",
    "mean_squared_error = MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DreamSim\n",
    "\n",
    "- To measure the perceptual similarity between two images\n",
    "- How? Concatenating CLIP, OpenCLIP, and DINO embeddings, and then finetuning on human perceptual judgements\n",
    "- Higher score means more different, lower means more similar\n",
    "- Link to [Notebook](https://colab.research.google.com/drive/1taEOMzFE9g81D9AwH27Uhy2U82tQGAVI?usp=sharing#scrollTo=zD2XN-UAvCZq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in ./models/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached ./models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "dreamsim_model, dreamsim_preprocess = dreamsim(pretrained=True, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(\\d+)-output(\\d+)'\n",
    "target_size = (1024, 1024)\n",
    "    \n",
    "def get_path(human_rating):\n",
    "    if human_rating==0:\n",
    "        path = \"../bad_samples\"\n",
    "    elif human_rating==1:\n",
    "        path = \"../good_samples\"\n",
    "    else: \n",
    "        print(\"Check Binary human rating, neither 0 nor 1!\")\n",
    "    return path\n",
    "    \n",
    "def get_mask_area(output_image, mask_image):\n",
    "    mask_image = mask_image.convert('RGB')\n",
    "    output_array = np.array(output_image)\n",
    "    mask_array = np.array(mask_image)\n",
    "    masked_area = cv2.absdiff(output_array, mask_array)\n",
    "    return masked_area\n",
    "\n",
    "def get_mask_area_image(output_image, mask_image):\n",
    "    masked_area = get_mask_area(output_image, mask_image)\n",
    "    return Image.fromarray(masked_area)\n",
    "\n",
    "def get_tensor(image):   \n",
    "    to_tensor = transforms.ToTensor()\n",
    "    return to_tensor(image).unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():   \n",
    "    results = []\n",
    "    for index,row in tqdm(samples.iterrows(), desc=\"Progress\"):\n",
    "        id = row[\"id\"] \n",
    "        turn = row[\"turn\"]\n",
    "\n",
    "        path = get_path(row[\"human_rating_binary\"])\n",
    "\n",
    "        for entry in turns:\n",
    "            output = entry[\"output\"]\n",
    "            match = re.search(pattern, output)\n",
    "\n",
    "            if match:\n",
    "                found_id = match.group(1) # get id of sample\n",
    "                found_turn = match.group(2) # get turn of sample\n",
    "\n",
    "                if int(found_id) == id and int(found_turn) == turn: # check if turn is within samples\n",
    "                    input = entry[\"input\"]\n",
    "                    mask = entry[\"mask\"]\n",
    "                    instruction = entry[\"instruction\"]\n",
    "\n",
    "                    output_image = Image.open(fr'{path}/{output}')\n",
    "                    input_image = Image.open(fr'{path}/{input}')\n",
    "                    mask_image = Image.open(fr'{path}/{mask}')\n",
    "                    #print(input_image.size)\n",
    "\n",
    "                    masked_area_image = get_mask_area_image(output_image, mask_image)\n",
    "                    masked_area_tensor = get_tensor(masked_area_image)\n",
    "                    \n",
    "                    #input_image = input_image.resize((1024, 1024))\n",
    "                    #initial_masked_area_image = get_mask_area_image(input_image, mask_image)\n",
    "                    #initial_masked_area_tensor = get_tensor(initial_masked_area_image)\n",
    "                    \n",
    "                    outside_mask_tensor = get_tensor(mask_image.convert('RGB'))\n",
    "                    output_image_tensor = get_tensor(output_image)\n",
    "                    full_img_tensor = F.interpolate(get_tensor(input_image), \n",
    "                                                            size=target_size, \n",
    "                                                            mode='bilinear', \n",
    "                                                            align_corners=False)\n",
    "                    \n",
    "                    print(get_tensor(input_image).shape)\n",
    "                    image = torch.cat([get_tensor(input_image), get_tensor(output_image)], axis=1)\n",
    "                    image_tensor = F.interpolate(image_tensor,\n",
    "                                                 size=target_size,\n",
    "                                                 mode='bilinear',\n",
    "                                                 align_corners=False)\n",
    "\n",
    "                    # CLIP\n",
    "                    clip_score_1 = clip(masked_area_tensor, instruction).detach().item()\n",
    "                    clip_score_2 = clip(output_image_tensor, instruction).detach().item()\n",
    "                    clip_score_3 = clip(image_tensor, instruction).detach().item()\n",
    "\n",
    "                    \"\"\"\n",
    "                    Calculate two different scores: \n",
    "                    - edited area & unedited area\n",
    "                    - edited area & original image\n",
    "                    (*See examples below)\n",
    "                    \"\"\"\n",
    "\n",
    "                    \"\"\"\n",
    "                    # LPIPS\n",
    "                    lpips_score_1 = lpips(masked_area_tensor, outside_mask_tensor).item()\n",
    "                    lpips_score_2 = lpips(masked_area_tensor, full_img_tensor).item()\n",
    "                    lpips_score_3 = lpips(full_img_tensor, output_image_tensor).item()\n",
    "                    lpips_score_4 = lpips(initial_masked_area_tensor, masked_area_tensor).item()\n",
    "\n",
    "                    # SSIM\n",
    "                    ssim_score_1 = ssim(masked_area_tensor, outside_mask_tensor).item()\n",
    "                    ssim_score_2 = ssim(masked_area_tensor, full_img_tensor).item()\n",
    "                    ssim_score_3 = ssim(full_img_tensor, output_image_tensor).item()\n",
    "                    ssim_score_4 = ssim(initial_masked_area_tensor, masked_area_tensor).item()\n",
    "\n",
    "                    # PSNR\n",
    "                    psnr_score_1 = psnr(masked_area_tensor, outside_mask_tensor).item()\n",
    "                    psnr_score_2 = psnr(masked_area_tensor, full_img_tensor).item()\n",
    "                    psnr_score_3 = psnr(full_img_tensor, output_image_tensor).item()\n",
    "                    psnr_score_4 = psnr(initial_masked_area_tensor, masked_area_tensor).item()\n",
    "\n",
    "                    # MAE\n",
    "                    mae_score_1 = mean_absolute_error(masked_area_tensor, outside_mask_tensor).item()\n",
    "                    mae_score_2 =  mean_absolute_error(masked_area_tensor, full_img_tensor).item()\n",
    "                    mae_score_3 =  mean_absolute_error(full_img_tensor, output_image_tensor).item()\n",
    "                    mae_score_4 =  mean_absolute_error(initial_masked_area_tensor, masked_area_tensor).item()\n",
    "\n",
    "                    # MSE\n",
    "                    mse_score_1 =  mean_squared_error(masked_area_tensor, outside_mask_tensor).item()       \n",
    "                    mse_score_2 =  mean_squared_error(masked_area_tensor, full_img_tensor).item()  \n",
    "                    mse_score_3 =  mean_squared_error(full_img_tensor, output_image_tensor).item()\n",
    "                    mse_score_4 =  mean_squared_error(initial_masked_area_tensor, masked_area_tensor).item()\n",
    "\n",
    "                    # DreamSim\n",
    "                    dream_sim_score_1 = dreamsim_model(\n",
    "                        dreamsim_preprocess(masked_area_image).to(\"cpu\"), \n",
    "                        dreamsim_preprocess(mask_image).to(\"cpu\")\n",
    "                    ).item()\n",
    "                    \n",
    "                    dream_sim_score_2 = dreamsim_model(\n",
    "                        dreamsim_preprocess(masked_area_image).to(\"cpu\"), \n",
    "                        dreamsim_preprocess(input_image).to(\"cpu\")\n",
    "                    ).item()\n",
    "                    \n",
    "                    dream_sim_score_3 = dreamsim_model(\n",
    "                        dreamsim_preprocess(input_image).to(\"cpu\"), \n",
    "                        dreamsim_preprocess(output_image).to(\"cpu\")\n",
    "                    ).item()\n",
    "                    \n",
    "                    dream_sim_score_4 = dreamsim_model(\n",
    "                        dreamsim_preprocess(initial_masked_area_image).to(\"cpu\"), \n",
    "                        dreamsim_preprocess(masked_area_image).to(\"cpu\")\n",
    "                    ).item()\n",
    "                    \"\"\"\n",
    "\n",
    "                    result = {\n",
    "                        \"index\": index,\n",
    "                        \"clip_score_1\": clip_score_1,\n",
    "                        \"clip_score_2\": clip_score_2,\n",
    "                        \"clip_score_3\": clip_score_3,\n",
    "                    }\n",
    "                    results.append(result)\n",
    "\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            if key != 'index':\n",
    "                samples.at[result['index'], key] = value\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'image_tensor' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mget_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mget_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_tensor(input_image)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     42\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([get_tensor(input_image), get_tensor(output_image)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\u001b[43mimage_tensor\u001b[49m,\n\u001b[1;32m     44\u001b[0m                              size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m     45\u001b[0m                              mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     46\u001b[0m                              align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# CLIP\u001b[39;00m\n\u001b[1;32m     49\u001b[0m clip_score_1 \u001b[38;5;241m=\u001b[39m clip(masked_area_tensor, instruction)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'image_tensor' referenced before assignment"
     ]
    }
   ],
   "source": [
    "samples = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>turn</th>\n",
       "      <th>human_rating_binary</th>\n",
       "      <th>clip_score_1</th>\n",
       "      <th>clip_score_2</th>\n",
       "      <th>lpips_score_1</th>\n",
       "      <th>lpips_score_2</th>\n",
       "      <th>lpips_score_3</th>\n",
       "      <th>ssim_score_1</th>\n",
       "      <th>ssim_score_2</th>\n",
       "      <th>...</th>\n",
       "      <th>mse_score_3</th>\n",
       "      <th>dream_sim_score_1</th>\n",
       "      <th>dream_sim_score_2</th>\n",
       "      <th>dream_sim_score_3</th>\n",
       "      <th>lpips_score_4</th>\n",
       "      <th>ssim_score_4</th>\n",
       "      <th>psnr_score_4</th>\n",
       "      <th>mae_score_4</th>\n",
       "      <th>mse_score_4</th>\n",
       "      <th>dream_sim_score_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22.089853</td>\n",
       "      <td>21.937899</td>\n",
       "      <td>0.743450</td>\n",
       "      <td>0.712001</td>\n",
       "      <td>0.090945</td>\n",
       "      <td>0.026027</td>\n",
       "      <td>0.101741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.684431</td>\n",
       "      <td>0.680819</td>\n",
       "      <td>0.049065</td>\n",
       "      <td>0.095240</td>\n",
       "      <td>0.925186</td>\n",
       "      <td>22.240963</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.404989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.171751</td>\n",
       "      <td>22.083504</td>\n",
       "      <td>0.748147</td>\n",
       "      <td>0.721401</td>\n",
       "      <td>0.143029</td>\n",
       "      <td>0.030369</td>\n",
       "      <td>0.251738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>0.698212</td>\n",
       "      <td>0.792064</td>\n",
       "      <td>0.023627</td>\n",
       "      <td>0.140604</td>\n",
       "      <td>0.926705</td>\n",
       "      <td>20.617954</td>\n",
       "      <td>0.026669</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>0.142823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5434</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20.117992</td>\n",
       "      <td>20.334154</td>\n",
       "      <td>0.705543</td>\n",
       "      <td>0.722557</td>\n",
       "      <td>0.154690</td>\n",
       "      <td>0.047790</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>0.598214</td>\n",
       "      <td>0.768621</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.181946</td>\n",
       "      <td>0.819952</td>\n",
       "      <td>17.881340</td>\n",
       "      <td>0.039803</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.289991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43425</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24.319836</td>\n",
       "      <td>24.311813</td>\n",
       "      <td>0.661717</td>\n",
       "      <td>0.663453</td>\n",
       "      <td>0.011401</td>\n",
       "      <td>0.060731</td>\n",
       "      <td>0.067517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.749138</td>\n",
       "      <td>0.780394</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.011233</td>\n",
       "      <td>0.988803</td>\n",
       "      <td>34.401665</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.081140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54492</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23.013315</td>\n",
       "      <td>23.010784</td>\n",
       "      <td>0.664067</td>\n",
       "      <td>0.682227</td>\n",
       "      <td>0.096621</td>\n",
       "      <td>0.038788</td>\n",
       "      <td>0.102482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011792</td>\n",
       "      <td>0.567202</td>\n",
       "      <td>0.652441</td>\n",
       "      <td>0.068155</td>\n",
       "      <td>0.088254</td>\n",
       "      <td>0.930151</td>\n",
       "      <td>19.289078</td>\n",
       "      <td>0.029084</td>\n",
       "      <td>0.011779</td>\n",
       "      <td>0.285989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>194956</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24.225899</td>\n",
       "      <td>24.088795</td>\n",
       "      <td>0.773829</td>\n",
       "      <td>0.784145</td>\n",
       "      <td>0.038071</td>\n",
       "      <td>0.039838</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.780690</td>\n",
       "      <td>0.812421</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.035032</td>\n",
       "      <td>0.976163</td>\n",
       "      <td>24.281298</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.286418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>203920</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23.103289</td>\n",
       "      <td>23.185959</td>\n",
       "      <td>0.662909</td>\n",
       "      <td>0.729869</td>\n",
       "      <td>0.304588</td>\n",
       "      <td>0.015874</td>\n",
       "      <td>0.220075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035316</td>\n",
       "      <td>0.581854</td>\n",
       "      <td>0.664295</td>\n",
       "      <td>0.286536</td>\n",
       "      <td>0.469181</td>\n",
       "      <td>0.675706</td>\n",
       "      <td>14.521969</td>\n",
       "      <td>0.083582</td>\n",
       "      <td>0.035302</td>\n",
       "      <td>0.524669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>209923</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.210674</td>\n",
       "      <td>21.257814</td>\n",
       "      <td>0.730553</td>\n",
       "      <td>0.738669</td>\n",
       "      <td>0.046077</td>\n",
       "      <td>0.080936</td>\n",
       "      <td>0.100084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.736350</td>\n",
       "      <td>0.776160</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.043388</td>\n",
       "      <td>0.957554</td>\n",
       "      <td>21.813662</td>\n",
       "      <td>0.014661</td>\n",
       "      <td>0.006586</td>\n",
       "      <td>0.288349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>211860</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.933147</td>\n",
       "      <td>22.661072</td>\n",
       "      <td>0.731956</td>\n",
       "      <td>0.726820</td>\n",
       "      <td>0.063587</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.022761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.707377</td>\n",
       "      <td>0.789326</td>\n",
       "      <td>0.108386</td>\n",
       "      <td>0.319897</td>\n",
       "      <td>0.832182</td>\n",
       "      <td>23.406586</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>0.306432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>211860</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.599344</td>\n",
       "      <td>20.823904</td>\n",
       "      <td>0.753795</td>\n",
       "      <td>0.747991</td>\n",
       "      <td>0.024872</td>\n",
       "      <td>0.017266</td>\n",
       "      <td>0.045610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.683527</td>\n",
       "      <td>0.745183</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.980684</td>\n",
       "      <td>28.802631</td>\n",
       "      <td>0.004735</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.081552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  turn  human_rating_binary  clip_score_1  clip_score_2  \\\n",
       "0       49     2                    0     22.089853     21.937899   \n",
       "1       49     3                    0     22.171751     22.083504   \n",
       "2     5434     3                    0     20.117992     20.334154   \n",
       "3    43425     2                    0     24.319836     24.311813   \n",
       "4    54492     2                    0     23.013315     23.010784   \n",
       "..     ...   ...                  ...           ...           ...   \n",
       "73  194956     2                    1     24.225899     24.088795   \n",
       "74  203920     1                    1     23.103289     23.185959   \n",
       "75  209923     3                    1     21.210674     21.257814   \n",
       "76  211860     1                    1     22.933147     22.661072   \n",
       "77  211860     2                    1     20.599344     20.823904   \n",
       "\n",
       "    lpips_score_1  lpips_score_2  lpips_score_3  ssim_score_1  ssim_score_2  \\\n",
       "0        0.743450       0.712001       0.090945      0.026027      0.101741   \n",
       "1        0.748147       0.721401       0.143029      0.030369      0.251738   \n",
       "2        0.705543       0.722557       0.154690      0.047790      0.106797   \n",
       "3        0.661717       0.663453       0.011401      0.060731      0.067517   \n",
       "4        0.664067       0.682227       0.096621      0.038788      0.102482   \n",
       "..            ...            ...            ...           ...           ...   \n",
       "73       0.773829       0.784145       0.038071      0.039838      0.066862   \n",
       "74       0.662909       0.729869       0.304588      0.015874      0.220075   \n",
       "75       0.730553       0.738669       0.046077      0.080936      0.100084   \n",
       "76       0.731956       0.726820       0.063587      0.000903      0.022761   \n",
       "77       0.753795       0.747991       0.024872      0.017266      0.045610   \n",
       "\n",
       "    ...  mse_score_3  dream_sim_score_1  dream_sim_score_2  dream_sim_score_3  \\\n",
       "0   ...     0.005973           0.684431           0.680819           0.049065   \n",
       "1   ...     0.008686           0.698212           0.792064           0.023627   \n",
       "2   ...     0.016799           0.598214           0.768621           0.034291   \n",
       "3   ...     0.000352           0.749138           0.780394           0.005919   \n",
       "4   ...     0.011792           0.567202           0.652441           0.068155   \n",
       "..  ...          ...                ...                ...                ...   \n",
       "73  ...     0.003676           0.780690           0.812421           0.005386   \n",
       "74  ...     0.035316           0.581854           0.664295           0.286536   \n",
       "75  ...     0.006593           0.736350           0.776160           0.009197   \n",
       "76  ...     0.004571           0.707377           0.789326           0.108386   \n",
       "77  ...     0.001321           0.683527           0.745183           0.036744   \n",
       "\n",
       "    lpips_score_4  ssim_score_4  psnr_score_4  mae_score_4  mse_score_4  \\\n",
       "0        0.095240      0.925186     22.240963     0.020217     0.005969   \n",
       "1        0.140604      0.926705     20.617954     0.026669     0.008674   \n",
       "2        0.181946      0.819952     17.881340     0.039803     0.016288   \n",
       "3        0.011233      0.988803     34.401665     0.001965     0.000349   \n",
       "4        0.088254      0.930151     19.289078     0.029084     0.011779   \n",
       "..            ...           ...           ...          ...          ...   \n",
       "73       0.035032      0.976163     24.281298     0.010720     0.003673   \n",
       "74       0.469181      0.675706     14.521969     0.083582     0.035302   \n",
       "75       0.043388      0.957554     21.813662     0.014661     0.006586   \n",
       "76       0.319897      0.832182     23.406586     0.013926     0.004564   \n",
       "77       0.022093      0.980684     28.802631     0.004735     0.001317   \n",
       "\n",
       "    dream_sim_score_4  \n",
       "0            0.404989  \n",
       "1            0.142823  \n",
       "2            0.289991  \n",
       "3            0.081140  \n",
       "4            0.285989  \n",
       "..                ...  \n",
       "73           0.286418  \n",
       "74           0.524669  \n",
       "75           0.288349  \n",
       "76           0.306432  \n",
       "77           0.081552  \n",
       "\n",
       "[78 rows x 29 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv(\"auto_scores_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \"\"\"\n",
    "                        \"lpips_score_1\": lpips_score_1,\n",
    "                        \"lpips_score_2\": lpips_score_2,\n",
    "                        \"lpips_score_3\": lpips_score_3,\n",
    "                        \"lpips_score_4\": lpips_score_4,\n",
    "                        \"ssim_score_1\": ssim_score_1,\n",
    "                        \"ssim_score_2\": ssim_score_2,\n",
    "                        \"ssim_score_3\": ssim_score_3,\n",
    "                        \"ssim_score_4\": ssim_score_4,\n",
    "                        \"psnr_score_1\": psnr_score_1,\n",
    "                        \"psnr_score_2\": psnr_score_2,\n",
    "                        \"psnr_score_3\": psnr_score_3,\n",
    "                        \"psnr_score_4\": psnr_score_4,\n",
    "                        \"mae_score_1\": mae_score_1,\n",
    "                        \"mae_score_2\": mae_score_2,\n",
    "                        \"mae_score_3\": mae_score_3,\n",
    "                        \"mae_score_4\": mae_score_4,\n",
    "                        \"mse_score_1\": mse_score_1,\n",
    "                        \"mse_score_2\": mse_score_2,\n",
    "                        \"mse_score_3\": mse_score_3,\n",
    "                        \"mse_score_4\": mse_score_4,\n",
    "                        \"dream_sim_score_1\": dream_sim_score_1,\n",
    "                        \"dream_sim_score_2\": dream_sim_score_2,\n",
    "                        \"dream_sim_score_3\": dream_sim_score_3,\n",
    "                        \"dream_sim_score_4\": dream_sim_score_4,\n",
    "                        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID: did not work as it requires multiple real images and multiple generated images\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # InceptionV3 expects 299x299\n",
    "    transforms.ToTensor()\n",
    "    transforms.Normalize(mean=[0, 0, 0], std=[1/255.0, 1/255.0, 1/255.0]),\n",
    "])\n",
    "\n",
    "#masked_image = Image.fromarray(cv2.cvtColor(masked_area, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "input_transformed = transform(input_image)\n",
    "masked_transformed = transform(masked_area_image)\n",
    "\n",
    "input_transformed = (input_transformed * 255).byte()\n",
    "masked_transformed = (masked_transformed * 255).byte()\n",
    "\n",
    "input_transformed = input_transformed.unsqueeze(0)\n",
    "masked_transformed = masked_transformed.unsqueeze(0)\n",
    "\n",
    "fid.update(input_transformed, real=True)\n",
    "fid.update(masked_transformed, real=False)\n",
    "fid_value = fid.compute()\n",
    "#samples.at[index, 'fid_value_1'] = fid_value\n",
    "print(fid_value)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
