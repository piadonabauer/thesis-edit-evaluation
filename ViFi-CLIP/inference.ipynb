{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff034b9-55b2-4d3b-b4a7-3a9c2f7225aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code for Performing Inference with ViFi-CLIP on Custom Videos\n",
    "\n",
    "This section provides code to perform inference with the **ViFi-CLIP** model on custom videos and instructions, allowing for similarity scoring based on specific instructions. It includes the initialization of the model, preprocessing pipeline, and functions for computing similarity scores between videos and instructions.\n",
    "\n",
    "- **Model Initialization and Preprocessing Pipeline**: The code sets up the ViFi-CLIP model and processes videos through a pipeline designed for the task.\n",
    "  \n",
    "- **Similarity Score Computation**: Given a video and an instruction, the code computes a similarity score to assess how well the modelâ€™s output aligns with the provided instruction.\n",
    "  \n",
    "- **Custom Configuration and Checkpoints**: Users can specify the configuration and checkpoint of the fine-tuned ViFi-CLIP model for inference.\n",
    "\n",
    "- **Scalability for DataFrames**: The code is designed to handle multiple videos by accepting a dataframe input, enabling scalability for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a87b3-c1e8-4654-8762-ffd033544482",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0b03bb-c850-4c95-bb3e-0bb381c3dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yacs ftfy timm regex -q\n",
    "!pip install mmcv-full -q\n",
    "!pip install decord -q\n",
    "!pip install opencv-python -q\n",
    "!pip install numpy==1.22.4 -q\n",
    "!pip install torch==2.6 -q\n",
    "\n",
    "# depending on possible errors\n",
    "#!pip install mmcv -q\n",
    "#!pip install mmengine -q\n",
    "#!pip install torch==1.11.0 -q\n",
    "#!pip install torchvision==0.12.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8d8a03-2bc5-4468-b682-eeb9f0fbcb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.config import get_config\n",
    "from utils.logger import create_logger\n",
    "import time\n",
    "import numpy as np\n",
    "from utils.config import get_config\n",
    "from trainers import vificlip\n",
    "from datasets.pipeline import *\n",
    "import warnings, logging\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b1a2f2-a091-4efd-9477-0d158e5b4b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3879a8-2450-4863-8b6a-e7f48501771f",
   "metadata": {},
   "source": [
    "### Setting up configuration\n",
    "Initializing requires specifying a configuration file and the pretrained model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78418d43-ddd6-460d-8c30-7884a6383c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "# Configuration class\n",
    "class parse_option:\n",
    "    def __init__(self, config, pretrained_model_path):\n",
    "        self.config = config\n",
    "        self.output = \"exp\"  # Name of output folder to store logs and save weights\n",
    "        self.resume = pretrained_model_path\n",
    "        # No need to change below args.\n",
    "        self.only_test = True\n",
    "        self.opts = None\n",
    "        self.batch_size = None\n",
    "        self.pretrained = None\n",
    "        self.accumulation_steps = None\n",
    "        self.local_rank = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda20686-733a-463b-8dbb-1b4bd1cb1eda",
   "metadata": {},
   "source": [
    "### Loading ViFi-CLIP and its pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d78f8b6-2e1c-4c0d-a196-9c9189f72f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(config, logger):\n",
    "    model = vificlip.returnCLIP(config, logger=logger)\n",
    "    model = model.float().cuda()\n",
    "\n",
    "    checkpoint = torch.load(config.MODEL.RESUME, map_location=\"cpu\", weights_only=False)\n",
    "    load_state_dict = checkpoint[\"model\"]\n",
    "\n",
    "    # remove unwanted keys\n",
    "    if \"module.prompt_learner.token_prefix\" in load_state_dict:\n",
    "        del load_state_dict[\"module.prompt_learner.token_prefix\"]\n",
    "\n",
    "    if \"module.prompt_learner.token_suffix\" in load_state_dict:\n",
    "        del load_state_dict[\"module.prompt_learner.token_suffix\"]\n",
    "\n",
    "    if \"module.prompt_learner.complete_text_embeddings\" in load_state_dict:\n",
    "        del load_state_dict[\"module.prompt_learner.complete_text_embeddings\"]\n",
    "\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in load_state_dict.items():\n",
    "        name = k[7:]  # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # load params\n",
    "    msg = model.load_state_dict(new_state_dict, strict=False)\n",
    "    logger.info(f\"resume model: {msg}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b0f3f-541c-4009-945f-977cc4032f0d",
   "metadata": {},
   "source": [
    "### Preprocessing input video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50308bb1-45e7-46d1-bf46-b2469dd2b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_preprocessing_pipeline(config):\n",
    "    # Preprocessing for video\n",
    "    img_norm_cfg = dict(\n",
    "        mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False\n",
    "    )\n",
    "    scale_resize = int(256 / 224 * config.DATA.INPUT_SIZE)\n",
    "    val_pipeline = [\n",
    "        dict(type=\"DecordInit\"),\n",
    "        dict(\n",
    "            type=\"SampleFrames\",\n",
    "            clip_len=1,\n",
    "            frame_interval=1,\n",
    "            num_clips=config.DATA.NUM_FRAMES,\n",
    "            test_mode=True,\n",
    "        ),\n",
    "        dict(type=\"DecordDecode\"),\n",
    "        dict(type=\"Resize\", scale=(-1, scale_resize)),\n",
    "        dict(type=\"CenterCrop\", crop_size=config.DATA.INPUT_SIZE),\n",
    "        dict(type=\"Normalize\", **img_norm_cfg),\n",
    "        dict(type=\"FormatShape\", input_format=\"NCHW\"),\n",
    "        dict(type=\"Collect\", keys=[\"imgs\"], meta_keys=[]),\n",
    "        dict(type=\"ToTensor\", keys=[\"imgs\"]),\n",
    "    ]\n",
    "    if config.TEST.NUM_CROP == 3:\n",
    "        val_pipeline[3] = dict(type=\"Resize\", scale=(-1, config.DATA.INPUT_SIZE))\n",
    "        val_pipeline[4] = dict(type=\"ThreeCrop\", crop_size=config.DATA.INPUT_SIZE)\n",
    "    if config.TEST.NUM_CLIP > 1:\n",
    "        val_pipeline[1] = dict(\n",
    "            type=\"SampleFrames\",\n",
    "            clip_len=1,\n",
    "            frame_interval=1,\n",
    "            num_clips=config.DATA.NUM_FRAMES,\n",
    "            multiview=config.TEST.NUM_CLIP,\n",
    "        )\n",
    "    pipeline = Compose(val_pipeline)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52bb28-c681-479a-89f3-fc86362b6d3f",
   "metadata": {},
   "source": [
    "### ViFi-CLIP inference with given video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e26ecb-5056-49a3-8102-3026a0206171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(video_path, instruction, model, pipeline):\n",
    "    dict_file = {\n",
    "        \"filename\": video_path,\n",
    "        \"tar\": False,\n",
    "        \"modality\": \"RGB\",\n",
    "        \"start_index\": 0,\n",
    "    }\n",
    "    video = pipeline(dict_file)\n",
    "    video_tensor = video[\"imgs\"].unsqueeze(0).cuda().float()\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(video_tensor, instruction)\n",
    "\n",
    "    if len(instruction) > 1:\n",
    "        return logits\n",
    "    else:\n",
    "        return logits.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ff5d48e-b696-468d-9e4e-a5364d11a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dev(config, model, saving_path, pipeline):\n",
    "    # adjust paths!\n",
    "    dev = pd.read_csv(\n",
    "        \"/home/jovyan/BA/Github/thesis-edit-evaluation/data/magicbrush/dev_data_with_mask.csv\"\n",
    "    )\n",
    "    magicbrush_dir = \"/home/jovyan/BA/Github/MagicBrush\"\n",
    "\n",
    "    results = []\n",
    "    for index, row in tqdm(dev.iterrows(), \"Predicting...\"):\n",
    "        # read paths for source image and target image with corresponding instruction\n",
    "        current_id = int(row[\"img_id\"])\n",
    "        current_turn = int(row[\"turn_index\"])\n",
    "        instruction = row[\"instruction\"].lower().replace(\".\", \"\")\n",
    "\n",
    "        # read number of frames from config file\n",
    "        frames = config.DATA.NUM_FRAMES\n",
    "\n",
    "        if frames == 2:\n",
    "            video_path = (\n",
    "                f\"{magicbrush_dir}/vifi_format/videos/{current_id}_{current_turn}.mp4\"\n",
    "            )\n",
    "        elif frames == 8:\n",
    "            video_path = f\"{magicbrush_dir}/vifi_format/videos_8_frames/{current_id}_{current_turn}.mp4\"\n",
    "\n",
    "        # predict similarity\n",
    "        similarity = predict_one(video_path, [instruction], model, pipeline)\n",
    "\n",
    "        row = {\n",
    "            \"id\": current_id,\n",
    "            \"turn\": current_turn,\n",
    "            \"score\": round(similarity, 3),\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "    vifi_scores = pd.DataFrame(results)\n",
    "    if saving_path:\n",
    "        vifi_scores.to_csv(saving_path, index=False)\n",
    "    return vifi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79cde364-c991-46f3-a9ab-1a812bfe38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(directory, save_dir):\n",
    "    config = directory + \"16_32_vifi_clip_all_shot.yaml\"\n",
    "    pretrained_model_path = directory + \"ckpt_epoch_10.pth\"\n",
    "\n",
    "    args = parse_option(config, pretrained_model_path)\n",
    "    config = get_config(args)\n",
    "    logger = create_logger(output_dir=args.output, name=f\"{config.MODEL.ARCH}\")\n",
    "\n",
    "    model = init_model(config, logger)\n",
    "    pipeline = init_preprocessing_pipeline(config)\n",
    "\n",
    "    df = predict_dev(config, model, save_dir, pipeline)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c2dfd-7b4f-4677-9846-d354347e9fdf",
   "metadata": {},
   "source": [
    "## Predict at Scale\n",
    "To perform predictions at scale for the entire validation split of MagicBrush, specify the folder containing the fine-tuned model path, as well as a name for the CSV file where the predictions should be saved afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47846e3c-8b1d-4c26-87d4-04149718ca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from /home/jovyan/BA/Github/thesis-edit-evaluation/ViFi-CLIP-og/output/few_shot/vitb16_2_frames/humanedit/2.5k_train_data/16_32_vifi_clip_all_shot.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting...: 528it [00:08, 60.56it/s]\n"
     ]
    }
   ],
   "source": [
    "base = \"/home/jovyan/BA/Github/thesis-edit-evaluation/\"\n",
    "\n",
    "df = compute_correlation(\n",
    "    base + \"ViFi-CLIP/output/crossvalidation/vitb16_2_humanedit_freeze_none/fold5/\",\n",
    "    base + \"labeling/analysis/cv2/vitb16_2_humanedit.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b8d75-043a-464b-816d-25f89d4919c6",
   "metadata": {},
   "source": [
    "## Single Inference\n",
    "Slightly modify the above-initialized setting to account for a single inference. Specification of a folder containing the checkpoints and config file is required, however, no path for saving anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2b4aadf-256e-4ea4-a908-fc0bc00fb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs(directory):\n",
    "    config = directory + \"16_32_vifi_clip_all_shot.yaml\"\n",
    "    pretrained_model_path = directory + \"ckpt_epoch_10.pth\"\n",
    "\n",
    "    args = parse_option(config, pretrained_model_path)\n",
    "    config = get_config(args)\n",
    "    logger = create_logger(output_dir=args.output, name=f\"{config.MODEL.ARCH}\")\n",
    "\n",
    "    model = init_model(config, logger)\n",
    "    pipeline = init_preprocessing_pipeline(config)\n",
    "    return model, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca0264ee-7191-49e1-9a2c-691614606639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from /home/jovyan/BA/Github/thesis-edit-evaluation/ViFi-CLIP/output/crossvalidation/vitb16_2_humanedit_freeze_none/fold1/16_32_vifi_clip_all_shot.yaml\n"
     ]
    }
   ],
   "source": [
    "base = \"/home/jovyan/BA/Github/thesis-edit-evaluation/\"\n",
    "\n",
    "model, pipeline = get_configs(\n",
    "    base + \"ViFi-CLIP/output/crossvalidation/vitb16_2_humanedit_freeze_none/fold1/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eea83bd8-dd15-4731-a052-6efd9563c36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23.8594, 18.6250, 21.5156, 19.2500, 21.0469]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"/home/jovyan/BA/Github/MagicBrush/vifi_format/videos/100626_1.mp4\"\n",
    "instruction = [\n",
    "    \"put a glass of soda on the table\",\n",
    "    \"let the bears sit on a leather couch\",\n",
    "    \"let's add a man in the kitchen\",\n",
    "    \"let there be a cup of yogurt\",\n",
    "    \"spill milk onto the floor\",\n",
    "]\n",
    "predict_one(video_path, instruction, model, pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (viFi-CLIP)",
   "language": "python",
   "name": "vifi-clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
